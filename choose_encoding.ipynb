{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5652e8-1f1a-411c-a2f1-c1ef2d93c55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import and global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943ef651-7fe0-492c-89a9-f665efcc2a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=2\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6af7db-52e6-46d4-806c-96981841fa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c5f770-0386-4422-b6e7-ad8fa3dc9ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable VRAM growth\n"
     ]
    }
   ],
   "source": [
    "def check_and_set_gpu():\n",
    "    gpu_list = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpu_list) == 0:\n",
    "        print(\"No available GPU!\")\n",
    "    else:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu_list[0], True)\n",
    "            print(\"Enable VRAM growth\")\n",
    "        except e:\n",
    "            print(e)\n",
    "check_and_set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8bfa5c-8ab2-419e-ac83-e6cd3c5037ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# for network debugging\n",
    "# tf.debugging.enable_check_numerics()\n",
    "# tf.debugging.experimental.enable_dump_debug_info(  #  incompatible with enable_op_determinism()\n",
    "#     f\"./logs/tfdbg2_logdir\",\n",
    "#     tensor_debug_mode=\"FULL_HEALTH\",\n",
    "#     circular_buffer_size=-1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a2ca6-d645-48db-b8ee-0867f99257ca",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed16530-65c3-4aa3-acec-66fbe0d5aa84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>productive</th>\n",
       "      <th>stop_codon</th>\n",
       "      <th>vj_in_frame</th>\n",
       "      <th>v_call</th>\n",
       "      <th>d_call</th>\n",
       "      <th>j_call</th>\n",
       "      <th>junction</th>\n",
       "      <th>junction_aa</th>\n",
       "      <th>...</th>\n",
       "      <th>d_sequence_start</th>\n",
       "      <th>d_sequence_end</th>\n",
       "      <th>j_sequence_start</th>\n",
       "      <th>j_sequence_end</th>\n",
       "      <th>shm_events</th>\n",
       "      <th>shm_count</th>\n",
       "      <th>shm_freq</th>\n",
       "      <th>unmutated_sequence</th>\n",
       "      <th>gapped_unmutated_sequence</th>\n",
       "      <th>gapped_mutated_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CAGGTGCAGCTGCGGGAGTCGGGCCCAGGGCTGGTGAAGCCTTTGG...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV4-61*08</td>\n",
       "      <td>IGHD3-3*02</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGCGCGAGGCCGCCAGGTGTATCAGCATTTAGGAGGACACCCGCTT...</td>\n",
       "      <td>CARPPGVSAFRRTPAWDFDPW</td>\n",
       "      <td>...</td>\n",
       "      <td>307</td>\n",
       "      <td>318</td>\n",
       "      <td>338</td>\n",
       "      <td>382</td>\n",
       "      <td>14:A&gt;G,30:A&gt;G,44:C&gt;T,68:C&gt;A,84:C&gt;T,89:G&gt;C,90:C...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.073298</td>\n",
       "      <td>CAGGTGCAGCTGCAGGAGTCGGGCCCAGGACTGGTGAAGCCTTCGG...</td>\n",
       "      <td>CAGGTGCAGCTGCAGGAGTCGGGCCCA...GGACTGGTGAAGCCTT...</td>\n",
       "      <td>CAGGTGCAGCTGCGGGAGTCGGGCCCA...GGGCTGGTGAAGCCTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CAGGTCACTTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV2-70*19</td>\n",
       "      <td>IGHD2-8*02</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCACGGGGGCATGTCCACGATAGGGTCTTTCCGAGAGTTGACT...</td>\n",
       "      <td>CARGHVHDRVFPRVDFW</td>\n",
       "      <td>...</td>\n",
       "      <td>310</td>\n",
       "      <td>313</td>\n",
       "      <td>329</td>\n",
       "      <td>370</td>\n",
       "      <td>9:C&gt;T,72:C&gt;T,82:T&gt;C,85:C&gt;A,88:A&gt;T,100:A&gt;G,105:...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>CAGGTCACCTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...</td>\n",
       "      <td>CAGGTCACCTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...</td>\n",
       "      <td>CAGGTCACTTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GAGGTGCAGCTCCCGGAGTCTGGGGGCGGCCTGGTACAGCCTGGGG...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV3-23*03</td>\n",
       "      <td>IGHD5-24*01</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCGAGAGACGGAAAAAAGAGACCCGACTGG</td>\n",
       "      <td>CARDGKKRPDW</td>\n",
       "      <td>...</td>\n",
       "      <td>305</td>\n",
       "      <td>309</td>\n",
       "      <td>314</td>\n",
       "      <td>349</td>\n",
       "      <td>12:G&gt;C,13:T&gt;C,14:T&gt;C,27:A&gt;C,31:T&gt;C,71:C&gt;T,90:C...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.097421</td>\n",
       "      <td>GAGGTGCAGCTGTTGGAGTCTGGGGGAGGCTTGGTACAGCCTGGGG...</td>\n",
       "      <td>GAGGTGCAGCTGTTGGAGTCTGGGGGA...GGCTTGGTACAGCCTG...</td>\n",
       "      <td>GAGGTGCAGCTCCCGGAGTCTGGGGGC...GGCCTGGTACAGCCTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGACCAGCCTGGGA...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV3-33*05</td>\n",
       "      <td>IGHD3/OR15-3a*01</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCGAGAGACAAAAATTTGGGACTGGCCGGGAACTTCTTTGACT...</td>\n",
       "      <td>CARDKNLGLAGNFFDYW</td>\n",
       "      <td>...</td>\n",
       "      <td>304</td>\n",
       "      <td>313</td>\n",
       "      <td>326</td>\n",
       "      <td>367</td>\n",
       "      <td>35:T&gt;A,72:G&gt;T,92:G&gt;C,98:G&gt;A,132:G&gt;A,151:A&gt;T,15...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.046322</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGTCCAGCCTGGGA...</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGTCCAGCCTG...</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGACCAGCCTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV3-74*02</td>\n",
       "      <td>IGHD1-26*01</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCAAGACAAGTGGGGGGCAATATCGACCACCTTTCGAAATACT...</td>\n",
       "      <td>CARQVGGNIDHLSKYYW</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>301</td>\n",
       "      <td>329</td>\n",
       "      <td>367</td>\n",
       "      <td>89:G&gt;C,93:C&gt;T,97:T&gt;C,119:C&gt;T,138:G&gt;A,147:A&gt;C,1...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence_id                                           sequence productive  \\\n",
       "0            0  CAGGTGCAGCTGCGGGAGTCGGGCCCAGGGCTGGTGAAGCCTTTGG...          T   \n",
       "1            1  CAGGTCACTTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...          T   \n",
       "2            2  GAGGTGCAGCTCCCGGAGTCTGGGGGCGGCCTGGTACAGCCTGGGG...          T   \n",
       "3            3  CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGACCAGCCTGGGA...          T   \n",
       "4            4  GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...          T   \n",
       "\n",
       "  stop_codon vj_in_frame       v_call            d_call    j_call  \\\n",
       "0          T           F  IGHV4-61*08        IGHD3-3*02  IGHJ4*01   \n",
       "1          T           F  IGHV2-70*19        IGHD2-8*02  IGHJ4*01   \n",
       "2          T           F  IGHV3-23*03       IGHD5-24*01  IGHJ4*01   \n",
       "3          T           F  IGHV3-33*05  IGHD3/OR15-3a*01  IGHJ4*01   \n",
       "4          T           F  IGHV3-74*02       IGHD1-26*01  IGHJ4*01   \n",
       "\n",
       "                                            junction            junction_aa  \\\n",
       "0  TGCGCGAGGCCGCCAGGTGTATCAGCATTTAGGAGGACACCCGCTT...  CARPPGVSAFRRTPAWDFDPW   \n",
       "1  TGTGCACGGGGGCATGTCCACGATAGGGTCTTTCCGAGAGTTGACT...      CARGHVHDRVFPRVDFW   \n",
       "2                  TGTGCGAGAGACGGAAAAAAGAGACCCGACTGG            CARDGKKRPDW   \n",
       "3  TGTGCGAGAGACAAAAATTTGGGACTGGCCGGGAACTTCTTTGACT...      CARDKNLGLAGNFFDYW   \n",
       "4  TGTGCAAGACAAGTGGGGGGCAATATCGACCACCTTTCGAAATACT...      CARQVGGNIDHLSKYYW   \n",
       "\n",
       "   ...  d_sequence_start  d_sequence_end j_sequence_start  j_sequence_end  \\\n",
       "0  ...               307             318              338             382   \n",
       "1  ...               310             313              329             370   \n",
       "2  ...               305             309              314             349   \n",
       "3  ...               304             313              326             367   \n",
       "4  ...               298             301              329             367   \n",
       "\n",
       "                                          shm_events  shm_count  shm_freq  \\\n",
       "0  14:A>G,30:A>G,44:C>T,68:C>A,84:C>T,89:G>C,90:C...         28  0.073298   \n",
       "1  9:C>T,72:C>T,82:T>C,85:C>A,88:A>T,100:A>G,105:...         20  0.054054   \n",
       "2  12:G>C,13:T>C,14:T>C,27:A>C,31:T>C,71:C>T,90:C...         34  0.097421   \n",
       "3  35:T>A,72:G>T,92:G>C,98:G>A,132:G>A,151:A>T,15...         17  0.046322   \n",
       "4  89:G>C,93:C>T,97:T>C,119:C>T,138:G>A,147:A>C,1...         14  0.038147   \n",
       "\n",
       "                                  unmutated_sequence  \\\n",
       "0  CAGGTGCAGCTGCAGGAGTCGGGCCCAGGACTGGTGAAGCCTTCGG...   \n",
       "1  CAGGTCACCTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...   \n",
       "2  GAGGTGCAGCTGTTGGAGTCTGGGGGAGGCTTGGTACAGCCTGGGG...   \n",
       "3  CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGTCCAGCCTGGGA...   \n",
       "4  GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...   \n",
       "\n",
       "                           gapped_unmutated_sequence  \\\n",
       "0  CAGGTGCAGCTGCAGGAGTCGGGCCCA...GGACTGGTGAAGCCTT...   \n",
       "1  CAGGTCACCTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...   \n",
       "2  GAGGTGCAGCTGTTGGAGTCTGGGGGA...GGCTTGGTACAGCCTG...   \n",
       "3  CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGTCCAGCCTG...   \n",
       "4  GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...   \n",
       "\n",
       "                             gapped_mutated_sequence  \n",
       "0  CAGGTGCAGCTGCGGGAGTCGGGCCCA...GGGCTGGTGAAGCCTT...  \n",
       "1  CAGGTCACTTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...  \n",
       "2  GAGGTGCAGCTCCCGGAGTCTGGGGGC...GGCCTGGTACAGCCTG...  \n",
       "3  CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGACCAGCCTG...  \n",
       "4  GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./data/airrship_shm_seed42_10_000.tsv\"\n",
    "df_data = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ae4531-c3f0-403f-b8b1-5472bd2f3c33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "v_sequence_start    True\n",
       "v_sequence_end      True\n",
       "d_sequence_start    True\n",
       "d_sequence_end      True\n",
       "j_sequence_start    True\n",
       "j_sequence_end      True\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_names = [f\"{seg}_sequence_{pos}\" for seg in \"vdj\" for pos in (\"start\", \"end\")]\n",
    "sequences = df_data[\"sequence\"]\n",
    "positions = df_data[pos_names]\n",
    "\n",
    "display(sequences.notna().all())\n",
    "display(positions.notna().all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54249d2f-1e4b-478a-98f4-25fbd944aabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'CAGGTGCAGCTGCGGGAGTCGGGCCCAGGGCTGGTGAAGCCTTTGGAGACCCTGTCCCTCACCTGCAATGTCTCTGGTGGCTCTGTCACTAGTGGTGGTTACTACTGGAGTTGGGTCCGGCTGACCCCAGGGAAGGGACTGGACTGGATTGGTTTTCTTTATTACAGTGGGAGTACCAATTACAACCCCTCCCTCGAGACTCGAGTCACCATATCAGTAGACACGGCCAAGAACCAGTTCTCTCTGAAGGTGAGCTCTGTGACCGCTGCGGACACGGCCGTGTATTACTGCGCGAGGCCGCCAGGTGTATCAGCATTTAGGAGGACACCCGCTTGGGACTTTGACCCCTGGGGCCATGGAACCCTGGTCACCGTCTCCTCAG'>,\n",
       " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  1, 296, 307, 318, 338, 382])>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_seq = Dataset.from_tensor_slices(sequences.to_numpy())\n",
    "ds_pos = Dataset.from_tensor_slices(positions.to_numpy())\n",
    "ds_all = Dataset.zip((ds_seq, ds_pos))\n",
    "\n",
    "display(ds_all.take(1).get_single_element())\n",
    "display(ds_all.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26440fe-c4fc-4d68-8bf2-49be52fba8e7",
   "metadata": {},
   "source": [
    "### Functions for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2078e6a0-d0ae-4f65-869f-3fe525925a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dna_onehot_tensor(seq):\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "        initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant([\"A\", \"C\", \"G\", \"T\"], dtype=tf.string),\n",
    "            values=tf.constant([0, 1, 2, 3]),\n",
    "        ),\n",
    "        default_value=tf.constant(-1)\n",
    "    )\n",
    "    chars = tf.strings.bytes_split(seq)\n",
    "    ind = table.lookup(chars)\n",
    "    encoded = tf.one_hot(ind, depth=4)\n",
    "    return encoded\n",
    "\n",
    "# test dna_onehot_tensor\n",
    "# test_oh = ds_all.take(6).map(\n",
    "#     lambda x, y: (dna_onehot_tensor(x), y)\n",
    "# )\n",
    "# for padded_batch in test_oh.padded_batch(3):\n",
    "#     display(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3ed857-9c79-49b2-8dc7-460ead6006b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kmer_tensor(seq, k):\n",
    "    chars = tf.strings.bytes_split(seq)\n",
    "    kmers = tf.strings.ngrams(chars, k, separator=\"\")\n",
    "    sentence = tf.strings.reduce_join(kmers, separator=\" \")\n",
    "    return sentence\n",
    "\n",
    "# test get_kmer_tensor()\n",
    "# test_kmer = ds_all.take(3).map(\n",
    "#     lambda x, y: (get_kmer_tensor(x, 3), y)\n",
    "# )\n",
    "# display(test_kmer.batch(3).take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062118ea-6e54-49ef-b761-82791c8180b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kmer_vocab(k):\n",
    "    return [\"\".join(x) for x in product(\"ACGT\", repeat=k)]\n",
    "\n",
    "# test get_kmer_vocab()\n",
    "# vocab_kmer = get_kmer_vocab(3)\n",
    "# \" \".join(vocab_kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946328b-cc7e-4889-ab25-23f5eafb237a",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d574e8c0-5ceb-4fed-a2a2-356de9878bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 8000\n",
    "VALID_SIZE = 1000\n",
    "TEST_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a7c30d1-cb2d-41b8-bd71-559c87c59513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_all.take(TRAIN_SIZE)\n",
    "ds_not_train = ds_all.skip(TRAIN_SIZE)\n",
    "ds_valid = ds_not_train.take(VALID_SIZE)\n",
    "ds_test = ds_not_train.skip(VALID_SIZE)\n",
    "\n",
    "assert ds_train.cardinality().numpy() == TRAIN_SIZE\n",
    "assert ds_valid.cardinality().numpy() == VALID_SIZE\n",
    "assert ds_test.cardinality().numpy() == TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd8b5b8-9dda-4e59-b2c1-600cf868930c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_ds(ds, method, scale_factor, batch_size=None, shuffle_buffer=None, shuffle_seed=None):\n",
    "    assert method == \"onehot\" or method.endswith(\"mer\")\n",
    "    assert (shuffle_buffer is not None) ^ (shuffle_seed is None)\n",
    "    if method == \"onehot\":\n",
    "        ds = ds.map(lambda x, y: (dna_onehot_tensor(x), y / scale_factor))\n",
    "    else:\n",
    "        k = int(method[0])\n",
    "        ds = ds.map(lambda x, y: (get_kmer_tensor(x, k), y / scale_factor))\n",
    "    if not batch_size:\n",
    "        return ds\n",
    "    if shuffle_buffer:\n",
    "        ds = ds.shuffle(shuffle_buffer, seed=shuffle_seed, reshuffle_each_iteration=True)\n",
    "    batched = ds.padded_batch(batch_size) if method == \"onehot\" else ds.batch(batch_size)\n",
    "    return batched\n",
    "                    \n",
    "# test encode_ds()\n",
    "# for batch_seq, batch_pos in transform_ds(ds_train.take(6), \"onehot\", 450, 3, 5, 1):\n",
    "#     display(batch_seq.shape)\n",
    "#     display(batch_pos.shape)   \n",
    "# display(batch_seq)\n",
    "# display(batch_pos)\n",
    "    \n",
    "# for batch_seq, batch_pos in transform_ds(ds_train.take(6), \"3mer\", 450, 3, 5, 1):\n",
    "#     display(batch_seq.shape)\n",
    "#     display(batch_pos.shape)\n",
    "# display(batch_seq)\n",
    "# display(batch_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fec84b-7b68-4494-9f60-9cb9fb30eaa0",
   "metadata": {},
   "source": [
    "### Functions and classes for building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d910fe37-2466-4a27-9330-d001711bd39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelWriteGrad(keras.Model):\n",
    "    \n",
    "    def set_model(\n",
    "        self, logdir, write_grad, \n",
    "        scalars=[], scalar_freq=1, histogram_freq=1, update_freq=\"batch\", batch_num=None, \n",
    "        inspect_adam=False, inspect_clip=False, replace_nan=None, replace_inf=None\n",
    "    ):\n",
    "        # Functions for compute scalars of gradients\n",
    "        self.scalar_funs = {\n",
    "            \"abs_max\": lambda x: tf.reduce_max(tf.abs(x)),\n",
    "            \"abs_min\": lambda x: tf.reduce_min(tf.abs(x)),\n",
    "            \"norm\": lambda x: tf.norm(x)\n",
    "        }\n",
    "        \n",
    "        # check arguments\n",
    "        assert set(scalars).issubset(set(self.scalar_funs.keys()))\n",
    "        assert update_freq in (\"batch\", \"epoch\")\n",
    "        assert not (update_freq == \"epoch\" and batch_num is None)\n",
    "        \n",
    "        # initialize\n",
    "        self.step = 0\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/gradients\") if write_grad else None\n",
    "        self.write_grad = write_grad\n",
    "        self.scalars = scalars\n",
    "        self.scalar_freq = scalar_freq if update_freq == \"batch\" else scalar_freq * batch_num\n",
    "        self.histogram_freq = histogram_freq if update_freq == \"batch\" else histogram_freq * batch_num\n",
    "        self.update_freq = update_freq\n",
    "        self.batch_num = batch_num\n",
    "        self.inspect_clip = inspect_clip\n",
    "        self.replace_nan = tf.constant(replace_nan, dtype=tf.float32) if replace_nan else False\n",
    "        self.replace_inf = tf.constant(replace_inf, dtype=tf.float32) if replace_inf else False\n",
    "        \n",
    "        opt_param = self.optimizer.get_config()\n",
    "        self.inspect_adam = (inspect_adam and opt_param[\"name\"] == \"Adam\")\n",
    "        if self.inspect_adam:\n",
    "            var_num = len(self.trainable_variables)\n",
    "            self.m = [0 for _ in range(var_num)]\n",
    "            self.v = [0 for _ in range(var_num)]\n",
    "            self.lr = opt_param[\"learning_rate\"]\n",
    "            self.beta_1 = opt_param[\"beta_1\"]\n",
    "            self.beta_2 = opt_param[\"beta_2\"]\n",
    "            self.epsilon = opt_param[\"epsilon\"]\n",
    "        if self.inspect_clip:\n",
    "            self.clipvalue = opt_param.get(\"clipvalue\")\n",
    "            self.clipnorm = opt_param.get(\"clipnorm\")\n",
    "            \n",
    "    def write_value(self, values, value_name, trainable_vars):\n",
    "        with self.writer.as_default():\n",
    "            for val, var in zip(values, trainable_vars):\n",
    "                if isinstance(val, tf.IndexedSlices):\n",
    "                    val = tf.convert_to_tensor(val)\n",
    "                var_name = var.name.replace(':', '_')\n",
    "                var_name = re.sub(r\"/gru_cell_\\d+\", \"\", var_name)\n",
    "                if self.histogram_freq != 0 and self.step % self.histogram_freq == 0:\n",
    "                    tf.summary.histogram(f\"{var_name}/{value_name}\", val, step=self.step)\n",
    "                if self.scalars and self.scalar_freq != 0 and self.step % self.scalar_freq == 0:\n",
    "                    for scalar in self.scalars:\n",
    "                        sum_name = f\"{var_name}/{value_name}/{scalar}\"\n",
    "                        sum_val = self.scalar_funs[scalar](val)\n",
    "                        tf.summary.scalar(sum_name, sum_val, step=self.step)\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def train_step(self, data):\n",
    "        \n",
    "        # Compute gradients\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Check and process nan and Inf\n",
    "        for i in range(len(gradients)):\n",
    "            grad = gradients[i]\n",
    "            var = trainable_vars[i]\n",
    "            var_name = var.name.replace(':', '_')\n",
    "            var_name = re.sub(r\"/gru_cell_\\d+\", \"\", var_name)\n",
    "            \n",
    "            nan_bool = tf.math.is_nan(grad)\n",
    "            if self.write_grad and self.inspect_clip:\n",
    "                show_nan = f\"Batch {self.step}: Nan in gradients of {var_name}\"\n",
    "                tf.cond(tf.reduce_any(nan_bool), lambda: tf.print(show_nan), tf.no_op)\n",
    "            if self.replace_nan:\n",
    "                if isinstance(grad, tf.IndexedSlices):\n",
    "                    grad = tf.convert_to_tensor(grad)\n",
    "                grad = tf.where(nan_bool, self.replace_nan, grad)\n",
    "                gradients[i] = grad\n",
    "            \n",
    "            inf_bool = tf.math.is_inf(grad)\n",
    "            if self.write_grad and self.inspect_clip:\n",
    "                show_inf = f\"Batch {self.step}: Inf in gradients of {var_name}\"\n",
    "                tf.cond(tf.reduce_any(inf_bool), lambda: tf.print(show_inf), tf.no_op)\n",
    "            if self.replace_inf:\n",
    "                if isinstance(grad, tf.IndexedSlices):\n",
    "                    grad = tf.convert_to_tensor(grad)\n",
    "                pos_bool = inf_bool & (grad > 0)\n",
    "                neg_bool = inf_bool & (grad < 0)\n",
    "                grad = tf.where(pos_bool, self.replace_inf, grad)\n",
    "                grad = tf.where(neg_bool, -1 * self.replace_inf, grad)\n",
    "                gradients[i] = grad\n",
    "                \n",
    "            if self.write_grad and self.inspect_clip and self.clipvalue:\n",
    "                show_clip = f\"Batch {self.step}: clip value for gradients of {var_name}\"\n",
    "                clip_bool = tf.reduce_any(tf.abs(grad) > self.clipvalue)\n",
    "                tf.cond(clip_bool, lambda: tf.print(show_clip), tf.no_op)\n",
    "            if self.write_grad and self.inspect_clip and self.clipnorm:\n",
    "                show_clip = f\"Batch {self.step}: clip norm for gradients of {var_name}\"\n",
    "                clip_bool = tf.reduce_any(tf.norm(grad) > self.clipnorm)\n",
    "                tf.cond(clip_bool, lambda: tf.print(show_clip), tf.no_op)\n",
    "        \n",
    "        # Compute adam values\n",
    "        if self.write_grad and self.inspect_adam:\n",
    "            t = self.step + 1\n",
    "            lr = self.lr * tf.sqrt(1 - tf.pow(self.beta_2, t)) / (1 - tf.pow(self.beta_1, t))\n",
    "            var_num = len(self.trainable_variables)\n",
    "            root_v = [0 for i in range(var_num)]\n",
    "            weight_updates = [0 for i in range(var_num)]\n",
    "            for i in range(var_num):\n",
    "                grad = gradients[i]\n",
    "                if isinstance(grad, tf.IndexedSlices):\n",
    "                    grad = tf.convert_to_tensor(grad)\n",
    "                self.m[i] = self.m[i] * self.beta_1 + (1 - self.beta_1) * grad\n",
    "                self.v[i] = self.v[i] * self.beta_2 + (1 - self.beta_2) * tf.square(grad)\n",
    "                root_v[i] = tf.sqrt(self.v[i])\n",
    "                weight_updates[i] = lr * self.m[i] / (root_v[i] + self.epsilon)\n",
    "        \n",
    "        # Record gradients\n",
    "        if self.write_grad:\n",
    "            self.write_value(gradients, \"grads\", trainable_vars)\n",
    "            if self.inspect_adam:\n",
    "                self.write_value(self.m, \"m\", trainable_vars)\n",
    "                self.write_value(root_v, \"root_v\", trainable_vars)\n",
    "                self.write_value(weight_updates, \"updates\", trainable_vars)\n",
    "                \n",
    "        # Update step, weights and metrics\n",
    "        self.step += 1\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72998de3-909a-4860-99c4-872693db0f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the encoders for kmer\n",
    "def create_kmer_encoder(k, embed_dim):\n",
    "    text_vec_layer = keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        split=\"whitespace\",\n",
    "        vocabulary=get_kmer_vocab(k),\n",
    "        name=\"text_vectorize\"\n",
    "    )\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        input_dim=text_vec_layer.vocabulary_size(), \n",
    "        output_dim=embed_dim, \n",
    "        mask_zero=True,\n",
    "        name=\"embedding\"\n",
    "    )\n",
    "\n",
    "    kmer_seq = keras.Input(shape=(1,), dtype=tf.string, name=f\"{k}mer_seq\")\n",
    "    tokens = text_vec_layer(kmer_seq)\n",
    "    embedding = embedding_layer(tokens)\n",
    "    kmer_encoder = keras.models.Model(kmer_seq, embedding, name=f\"{k}mer_encoder\")\n",
    "    return kmer_encoder\n",
    "    \n",
    "# test_encoder = create_kmer_encoder(5, 64)\n",
    "# test_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c9cc3d1-d5f2-4436-abb3-227db473f61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the complete models\n",
    "def create_model(\n",
    "    encode_name, *, \n",
    "    EMBED_DIM, GRU_NUM, GRU_UNIT, GRU_BIDIRECT, DENSE_NUM, DENSE_UNIT, LEAKY_ALPHA, OUT_ACTI\n",
    "):\n",
    "    \n",
    "    # input and encoding part\n",
    "    if encode_name == \"onehot\":\n",
    "        model_input = keras.Input(shape=(None, 4), name=\"input\")\n",
    "        x = keras.layers.Masking(mask_value=[0.0, 0.0, 0.0, 0.0], name=\"mask\")(model_input)\n",
    "    else:\n",
    "        k = int(encode_name[0])\n",
    "        model_input = keras.Input(shape=(1, ), dtype=tf.string, name=\"input\")\n",
    "        kmer_encoder = create_kmer_encoder(k, EMBED_DIM)\n",
    "        x = kmer_encoder(model_input)\n",
    "    \n",
    "    # gru part\n",
    "    for i in range(GRU_NUM):\n",
    "        gru_bidirect = GRU_BIDIRECT if isinstance(GRU_BIDIRECT, bool) else GRU_BIDIRECT[i]\n",
    "        gru_unit = GRU_UNIT if isinstance(GRU_UNIT, int) else GRU_UNIT[i]\n",
    "        not_last_gru = (i+1 != GRU_NUM)\n",
    "        gru_layer = keras.layers.GRU(gru_unit, return_sequences=not_last_gru, name=f\"gru_{i+1}\")\n",
    "        if gru_bidirect:\n",
    "            gru_layer = keras.layers.Bidirectional(gru_layer, name=f\"gru_{i+1}\")\n",
    "        x = gru_layer(x)\n",
    "    \n",
    "    # dense part\n",
    "    dense_acti = lambda x: keras.activations.relu(x, alpha=LEAKY_ALPHA)\n",
    "    for i in range(DENSE_NUM):\n",
    "        dense_unit = DENSE_UNIT if isinstance(DENSE_UNIT, int) else DENSE_UNIT[i]\n",
    "        x = keras.layers.Dense(dense_unit, activation=dense_acti, name=f\"dense_{i+1}\")(x)\n",
    "    \n",
    "    # build the model\n",
    "    model_output = keras.layers.Dense(6, activation=OUT_ACTI, name=\"output\")(x)\n",
    "    model = ModelWriteGrad(model_input, model_output, name=f\"{encode_name}_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e3078-0953-4538-8305-c928dea890fe",
   "metadata": {},
   "source": [
    "### Build model, compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fe5ec4-c48b-4742-bb32-456c9064dddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch_num(ds_size, batch_size):\n",
    "    a, b = divmod(ds_size, batch_size)\n",
    "    return a + int(b != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4928c433-41a7-4e99-afae-bae89ca5c229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretty_json(hp):\n",
    "  json_hp = json.dumps(hp, indent=2)\n",
    "  return \"\".join(\"\\t\" + line for line in json_hp.splitlines(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee95f8c7-745b-4df9-9c0e-120df039012f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom metrics\n",
    "class ScaledRMSE(keras.metrics.RootMeanSquaredError):\n",
    "    def __init__(self, scale_factor, name=\"s_rmse\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, self._dtype) * self.scale_factor\n",
    "        y_pred = tf.cast(y_pred, self._dtype) * self.scale_factor\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class ScaledMAE(keras.metrics.MeanAbsoluteError):\n",
    "    def __init__(self, scale_factor, name=\"s_mae\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, self._dtype) * self.scale_factor\n",
    "        y_pred = tf.cast(y_pred, self._dtype) * self.scale_factor\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class PosAccuracy(keras.metrics.Accuracy):\n",
    "    def __init__(self, scale_factor, name=\"p_acc\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.math.rint(tf.cast(y_pred, self._dtype) * self.scale_factor)\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class AllAccuracy(keras.metrics.Accuracy):\n",
    "    def __init__(self, scale_factor, name=\"a_acc\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.math.rint(tf.cast(y_pred, self._dtype) * self.scale_factor)\n",
    "        all_correct_bool = tf.reduce_all(y_true == y_pred, axis=-1)\n",
    "        correct_mat = tf.where(all_correct_bool, 1, 0)\n",
    "        y_pred = tf.reshape(correct_mat, (-1, 1))\n",
    "        y_true = tf.ones(tf.shape(y_true)[0], 1)\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class ClassAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, scale_factor, name=\"c_acc\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.len_sum = self.add_weight(name=\"len_sum\", initializer=\"zeros\")\n",
    "        self.correct_sum = self.add_weight(name=\"correct_sum\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.math.rint(tf.cast(y_pred, self._dtype) * self.scale_factor)\n",
    "        incorrect_count = tf.reduce_sum(tf.abs(y_true - y_pred))\n",
    "        batch_len_sum = tf.reduce_sum(tf.gather(y_true, indices=(5), axis=-1))\n",
    "        self.len_sum.assign_add(batch_len_sum)\n",
    "        self.correct_sum.assign_add(batch_len_sum - incorrect_count)\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_sum / self.len_sum\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.len_sum.assign(0.0)\n",
    "        self.correct_sum.assign(0.0)\n",
    "        \n",
    "class SegmentPosAccuracy(PosAccuracy):\n",
    "    def __init__(self, scale_factor, segment, name=\"p_acc\", dtype=None):\n",
    "        assert segment in (\"V\", \"D\", \"J\")\n",
    "        super().__init__(scale_factor, name=f\"{segment}_{name}\", dtype=dtype)\n",
    "        self.segment = segment\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        seg_idx_dict = {\"V\": (0, 1), \"D\": (2, 3), \"J\": (4, 5)}\n",
    "        seg_idx = seg_idx_dict[self.segment]\n",
    "        y_true_seg = tf.gather(y_true, indices=seg_idx, axis=-1)\n",
    "        y_pred_seg = tf.gather(y_pred, indices=seg_idx, axis=-1)\n",
    "        super().update_state(y_true_seg, y_pred_seg, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3cb6c61-2437-4912-a79c-9af101ae6b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A callback class that stops the training when there is a sudden increase in one of the metrics \n",
    "class SpikeStopping(keras.callbacks.Callback):\n",
    "    def __init__(self, monitor, fold_allow=2, min_allow=5, skip_first=10):\n",
    "        super(SpikeStopping, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.fold_allow = fold_allow\n",
    "        self.min_allow = min_allow\n",
    "        self.skip_first = skip_first\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if epoch == 0:\n",
    "            self.last = current\n",
    "        if epoch >= self.skip_first and current > self.last * fold_allow + min_allow:\n",
    "            print(f\"Stop training at epoch {epoch}\")\n",
    "            self.model.stop_training = True\n",
    "        self.last = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e1ef42a-d2dc-4dc5-bc0c-b007888fa230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A callback class that records the training information\n",
    "class WriteInfo(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir, info):\n",
    "        super().__init__()\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/info\")\n",
    "        self.info = info\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        model_info = {\n",
    "            \"MODEL_NAME\": self.model.name,\n",
    "            \"MODEL_PARAM_NUM\": self.model.count_params(),\n",
    "        }\n",
    "        self.info[\"MODEL_INFO\"] = model_info\n",
    "        info_json = pretty_json(self.info)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.text(\"training_setting\", info_json, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dd17fa8-8840-4ef4-b322-78f3eb287ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A callback class that records the training time\n",
    "class WriteTrainingTime(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir, print_time=True):\n",
    "        super().__init__()\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/info\")\n",
    "        self.print_time = print_time\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start = datetime.now()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        duration = datetime.now() - self.start + datetime.min\n",
    "        time_str = duration.strftime(\"%H:%M:%S.%f\")\n",
    "        if self.print_time:\n",
    "            print(f\"Time usage: {time_str}\")\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.text(\"training_time\", time_str, step=self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1dceabc-8fef-46e7-ab40-62e3fdb9d780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRU_UNIT</th>\n",
       "      <th>GRU_NUM</th>\n",
       "      <th>GRU_BIDIRECT</th>\n",
       "      <th>DENSE_UNIT</th>\n",
       "      <th>DENSE_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GRU_UNIT GRU_NUM GRU_BIDIRECT DENSE_UNIT DENSE_NUM\n",
       "1     1024       1        False        128         4\n",
       "2      512       1         True        128         4\n",
       "3      256       6        False        256         1\n",
       "4      128       6         True        256         1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "    \"GRU_UNIT\": [1024, 512, 256, 128],\n",
    "    \"GRU_NUM\": [1, 1, 6, 6], \n",
    "    \"GRU_BIDIRECT\": [False, True, False, True], \n",
    "    \"DENSE_UNIT\": [128, 128, 256, 256], \n",
    "    \"DENSE_NUM\": [4, 4, 1, 1]\n",
    "}\n",
    "df_settings = pd.DataFrame(settings, index=range(1, 5), dtype=\"object\")\n",
    "df_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab5e88d-9965-44ab-bf7f-720c062ced81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"5mer_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " 5mer_encoder (Functional)   (None, None, 64)          65664     \n",
      "                                                                 \n",
      " gru_1 (Bidirectional)       (None, None, 256)         148992    \n",
      "                                                                 \n",
      " gru_2 (Bidirectional)       (None, None, 256)         296448    \n",
      "                                                                 \n",
      " gru_3 (Bidirectional)       (None, None, 256)         296448    \n",
      "                                                                 \n",
      " gru_4 (Bidirectional)       (None, None, 256)         296448    \n",
      "                                                                 \n",
      " gru_5 (Bidirectional)       (None, None, 256)         296448    \n",
      "                                                                 \n",
      " gru_6 (Bidirectional)       (None, 256)               296448    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " output (Dense)              (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,764,230\n",
      "Trainable params: 1,764,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SETTING = 4\n",
    "MODEL_STRUCT = {\n",
    "    \"EMBED_DIM\": 64,\n",
    "    \"GRU_UNIT\": df_settings.loc[SETTING, \"GRU_UNIT\"],\n",
    "    \"GRU_NUM\": df_settings.loc[SETTING, \"GRU_NUM\"],\n",
    "    \"GRU_BIDIRECT\": df_settings.loc[SETTING, \"GRU_BIDIRECT\"],\n",
    "    \"DENSE_UNIT\": df_settings.loc[SETTING, \"DENSE_UNIT\"],\n",
    "    \"DENSE_NUM\": df_settings.loc[SETTING, \"DENSE_NUM\"],\n",
    "    \"LEAKY_ALPHA\": 0.1,\n",
    "    \"OUT_ACTI\": \"linear\"\n",
    "}\n",
    "show_model = create_model(\"5mer\", **MODEL_STRUCT)\n",
    "show_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a079a205-efeb-4425-8a95-0f52e367ceab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training setting\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 32\n",
    "SCALE_FACTOR = 450\n",
    "SHUFFLE_BUFFER = 128\n",
    "PREFETCH_BUFFER = 8\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "OPTIMIZER = keras.optimizers.Adam(\n",
    "    learning_rate=1e-3,\n",
    "    clipnorm=1e-3\n",
    ")\n",
    "REPLACE_NAN = 1e-5\n",
    "REPLACE_INF = 1e-5\n",
    "EARLY_STOP = False\n",
    "STOP_NAN = True\n",
    "WRITE_GRAD = False\n",
    "\n",
    "train_info = {\n",
    "    \"SEED\": SEED,\n",
    "    \"DATA\": {\n",
    "        \"DATA_PATH\": DATA_PATH,\n",
    "        \"TRAIN_SIZE\": TRAIN_SIZE,\n",
    "        \"VALID_SIZE\": VALID_SIZE,\n",
    "        \"TEST_SIZE\": TEST_SIZE,\n",
    "        \"SCALE_FACTOR\": SCALE_FACTOR,\n",
    "        \"SHUFFLE_BUFFER\": SHUFFLE_BUFFER,\n",
    "        \"PREFETCH_BUFFER\": PREFETCH_BUFFER\n",
    "    },\n",
    "    \"MODEL_STRUCT\": MODEL_STRUCT,\n",
    "    \"TRAIN_SET\": {\n",
    "        \"EPOCH\": EPOCH,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LOSS\": LOSS.get_config(),\n",
    "        \"OPTIMIZER\": OPTIMIZER.get_config(),\n",
    "        \"REPLACE_NAN\": REPLACE_NAN,\n",
    "        \"REPLACE_INF\": REPLACE_INF,\n",
    "        \"EARLY_STOP\": EARLY_STOP,\n",
    "        \"STOP_NAN\": STOP_NAN,\n",
    "        \"WRITE_GRAD\": WRITE_GRAD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d1a77-d5c4-4319-909a-640ec0fc1ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onehot_model training!\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "timestamp = datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "MAIN_LOGDIR = f\"./logs/ce_no_spike/repro_setting_{SETTING}_seed_{SEED}_3\"\n",
    "\n",
    "encode_names = (\"onehot\", \"2mer\", \"3mer\", \"4mer\", \"5mer\")\n",
    "metric_list = [\n",
    "    ScaledRMSE(SCALE_FACTOR),\n",
    "    ScaledMAE(SCALE_FACTOR),\n",
    "    PosAccuracy(SCALE_FACTOR),\n",
    "    AllAccuracy(SCALE_FACTOR),\n",
    "    ClassAccuracy(SCALE_FACTOR),\n",
    "    SegmentPosAccuracy(SCALE_FACTOR, segment=\"V\"),\n",
    "    SegmentPosAccuracy(SCALE_FACTOR, segment=\"D\"),\n",
    "    SegmentPosAccuracy(SCALE_FACTOR, segment=\"J\")\n",
    "]\n",
    "batch_num = get_batch_num(TRAIN_SIZE, BATCH_SIZE)\n",
    "\n",
    "all_history = {}\n",
    "for name in encode_names:\n",
    "    \n",
    "    # build model and compile\n",
    "    model = create_model(name, **MODEL_STRUCT)\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=metric_list, run_eagerly=WRITE_GRAD)\n",
    "    \n",
    "    # prepare dataset\n",
    "    ds_train_input = transform_ds(ds_train, name, SCALE_FACTOR, BATCH_SIZE, SHUFFLE_BUFFER, SEED)\n",
    "    ds_valid_input = transform_ds(ds_valid, name, SCALE_FACTOR, BATCH_SIZE)\n",
    "    ds_train_input = ds_train_input.prefetch(PREFETCH_BUFFER)\n",
    "    ds_valid_input = ds_valid_input.prefetch(PREFETCH_BUFFER)\n",
    "    \n",
    "    # setting callbacks\n",
    "    logdir = MAIN_LOGDIR + f\"/{model.name}\"\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "        logdir, \n",
    "        histogram_freq=max(1, EPOCH // 20),\n",
    "        write_graph=False\n",
    "    )\n",
    "    # spike_stop_cb = SpikeStopping(\"s_rmse\", fold_allow=1.5, min_allow=3, skip_first=10)\n",
    "    write_info_cb = WriteInfo(logdir, train_info)\n",
    "    write_time_cb = WriteTrainingTime(logdir)\n",
    "    cb_list = [\n",
    "        tensorboard_cb,\n",
    "        # spike_stop_cb,\n",
    "        write_info_cb,\n",
    "        write_time_cb\n",
    "    ]\n",
    "    if STOP_NAN:\n",
    "        stop_nan_cb = keras.callbacks.TerminateOnNaN()\n",
    "        cb_list.append(stop_nan_cb)\n",
    "    if EARLY_STOP:\n",
    "        early_stop_cb = keras.callbacks.EarlyStopping(**EARLY_STOP)\n",
    "        cb_list.append(early_stop_cb)\n",
    "    \n",
    "    # train\n",
    "    print(f\"{model.name} training!\")\n",
    "    model.set_model(\n",
    "        logdir, write_grad=WRITE_GRAD,\n",
    "        scalars=[\"norm\"], scalar_freq=0, histogram_freq=0, update_freq=\"epoch\", batch_num=batch_num,\n",
    "        inspect_adam=False, inspect_clip=False, replace_nan=REPLACE_NAN, replace_inf=REPLACE_INF\n",
    "    )\n",
    "    all_history[name] = model.fit(\n",
    "        ds_train_input,\n",
    "        epochs=EPOCH, \n",
    "        validation_data=ds_valid_input,\n",
    "        callbacks=cb_list,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.save(logdir + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff764c-8354-4faa-a41d-3a7ba900a0c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bbbb1-c29e-44f0-b0b9-d712a20168ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance = pd.DataFrame()\n",
    "for i, name in enumerate(encode_names):\n",
    "    df = pd.DataFrame(all_history[name].history)\n",
    "    best_loss_idx = df[\"loss\"].argmin()\n",
    "    best_val_loss_idx = df[\"val_loss\"].argmin()\n",
    "    valid_metrics_idx = df.columns.str.startswith(\"val\")\n",
    "    train_metrics = df.columns[~valid_metrics_idx]\n",
    "    valid_metrics = df.columns[valid_metrics_idx]\n",
    "    performance.loc[i, \"name\"] = name\n",
    "    performance.loc[i, \"number_of_parameters\"] = all_history[name].model.count_params()\n",
    "    performance.loc[i, \"best_loss_epoch\"] = df.index[best_loss_idx]\n",
    "    performance.loc[i, train_metrics] = df.loc[best_loss_idx, train_metrics]\n",
    "    performance.loc[i, \"best_val_loss_epoch\"] = df.index[best_val_loss_idx]\n",
    "    performance.loc[i, valid_metrics] = df.loc[best_loss_idx, valid_metrics]\n",
    "\n",
    "performance = pd.DataFrame(performance)\n",
    "performance.to_csv(MAIN_LOGDIR + \"/performance.csv\", index=False)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38eb29-f248-48d7-9a97-278bff60792b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name in encode_names:\n",
    "#     print(name)\n",
    "#     model = all_history[name].model\n",
    "#     ds_test_input = transform_ds(ds_test, name, SCALE_FACTOR, BATCH_SIZE)\n",
    "#     model.evaluate(ds_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5d8fc-7069-49d8-8dbc-ad5b0862ad57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DATA_PATH.endswith(\"10_000_processed.tsv\"):\n",
    "#     print(\"Train on random ends, test on not random ends\")\n",
    "#     df_other = pd.read_csv(\"./data/airrship_shm_seed42_10_000.tsv\", sep=\"\\t\")\n",
    "# else:\n",
    "#     print(\"Train on not random ends, test on random ends\")\n",
    "#     df_other = pd.read_csv(\"./data/airrship_shm_seed42_10_000_processed.tsv\", sep=\"\\t\")\n",
    "    \n",
    "# df_other = df_other.iloc[-1000:]\n",
    "# print(df_other.shape)\n",
    "\n",
    "# pos_names = [f\"{seg}_sequence_{pos}\" for seg in \"vdj\" for pos in (\"start\", \"end\")]\n",
    "# seq_other = df_other[\"sequence\"]\n",
    "# pos_other = df_other[pos_names]\n",
    "\n",
    "# ds_seq_other = Dataset.from_tensor_slices(seq_other.to_numpy())\n",
    "# ds_pos_other = Dataset.from_tensor_slices(pos_other.to_numpy())\n",
    "# ds_all_other = Dataset.zip((ds_seq_other, ds_pos_other))\n",
    "\n",
    "# for name in encode_names:\n",
    "#     print(name)\n",
    "#     model = all_history[name].model\n",
    "#     ds_other = transform_ds(ds_all_other, name, SCALE_FACTOR, BATCH_SIZE)\n",
    "#     model.evaluate(ds_other)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf210",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
