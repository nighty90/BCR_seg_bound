{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5652e8-1f1a-411c-a2f1-c1ef2d93c55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import and global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943ef651-7fe0-492c-89a9-f665efcc2a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=2\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6af7db-52e6-46d4-806c-96981841fa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import re\n",
    "import json\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c5f770-0386-4422-b6e7-ad8fa3dc9ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable VRAM growth\n"
     ]
    }
   ],
   "source": [
    "def check_and_set_gpu():\n",
    "    gpu_list = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpu_list) == 0:\n",
    "        print(\"No available GPU!\")\n",
    "    else:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu_list[0], True)\n",
    "            print(\"Enable VRAM growth\")\n",
    "        except e:\n",
    "            print(e)\n",
    "check_and_set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8bfa5c-8ab2-419e-ac83-e6cd3c5037ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# for network debugging\n",
    "# tf.debugging.enable_check_numerics()\n",
    "# tf.debugging.experimental.enable_dump_debug_info(  #  incompatible with enable_op_determinism()\n",
    "#     f\"./logs/tfdbg2_logdir\",\n",
    "#     tensor_debug_mode=\"FULL_HEALTH\",\n",
    "#     circular_buffer_size=-1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a2ca6-d645-48db-b8ee-0867f99257ca",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed16530-65c3-4aa3-acec-66fbe0d5aa84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>productive</th>\n",
       "      <th>stop_codon</th>\n",
       "      <th>vj_in_frame</th>\n",
       "      <th>v_call</th>\n",
       "      <th>d_call</th>\n",
       "      <th>j_call</th>\n",
       "      <th>junction</th>\n",
       "      <th>junction_aa</th>\n",
       "      <th>...</th>\n",
       "      <th>d_sequence_start</th>\n",
       "      <th>d_sequence_end</th>\n",
       "      <th>j_sequence_start</th>\n",
       "      <th>j_sequence_end</th>\n",
       "      <th>shm_events</th>\n",
       "      <th>shm_count</th>\n",
       "      <th>shm_freq</th>\n",
       "      <th>unmutated_sequence</th>\n",
       "      <th>gapped_unmutated_sequence</th>\n",
       "      <th>gapped_mutated_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CAGGTGCAGCTGCGGGAGTCGGGCCCAGGGCTGGTGAAGCCTTTGG...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV4-61*08</td>\n",
       "      <td>IGHD3-3*02</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGCGCGAGGCCGCCAGGTGTATCAGCATTTAGGAGGACACCCGCTT...</td>\n",
       "      <td>CARPPGVSAFRRTPAWDFDPW</td>\n",
       "      <td>...</td>\n",
       "      <td>307</td>\n",
       "      <td>318</td>\n",
       "      <td>338</td>\n",
       "      <td>382</td>\n",
       "      <td>14:A&gt;G,30:A&gt;G,44:C&gt;T,68:C&gt;A,84:C&gt;T,89:G&gt;C,90:C...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.073298</td>\n",
       "      <td>CAGGTGCAGCTGCAGGAGTCGGGCCCAGGACTGGTGAAGCCTTCGG...</td>\n",
       "      <td>CAGGTGCAGCTGCAGGAGTCGGGCCCA...GGACTGGTGAAGCCTT...</td>\n",
       "      <td>CAGGTGCAGCTGCGGGAGTCGGGCCCA...GGGCTGGTGAAGCCTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CAGGTCACTTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV2-70*19</td>\n",
       "      <td>IGHD2-8*02</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCACGGGGGCATGTCCACGATAGGGTCTTTCCGAGAGTTGACT...</td>\n",
       "      <td>CARGHVHDRVFPRVDFW</td>\n",
       "      <td>...</td>\n",
       "      <td>310</td>\n",
       "      <td>313</td>\n",
       "      <td>329</td>\n",
       "      <td>370</td>\n",
       "      <td>9:C&gt;T,72:C&gt;T,82:T&gt;C,85:C&gt;A,88:A&gt;T,100:A&gt;G,105:...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>CAGGTCACCTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...</td>\n",
       "      <td>CAGGTCACCTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...</td>\n",
       "      <td>CAGGTCACTTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GAGGTGCAGCTCCCGGAGTCTGGGGGCGGCCTGGTACAGCCTGGGG...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV3-23*03</td>\n",
       "      <td>IGHD5-24*01</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCGAGAGACGGAAAAAAGAGACCCGACTGG</td>\n",
       "      <td>CARDGKKRPDW</td>\n",
       "      <td>...</td>\n",
       "      <td>305</td>\n",
       "      <td>309</td>\n",
       "      <td>314</td>\n",
       "      <td>349</td>\n",
       "      <td>12:G&gt;C,13:T&gt;C,14:T&gt;C,27:A&gt;C,31:T&gt;C,71:C&gt;T,90:C...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.097421</td>\n",
       "      <td>GAGGTGCAGCTGTTGGAGTCTGGGGGAGGCTTGGTACAGCCTGGGG...</td>\n",
       "      <td>GAGGTGCAGCTGTTGGAGTCTGGGGGA...GGCTTGGTACAGCCTG...</td>\n",
       "      <td>GAGGTGCAGCTCCCGGAGTCTGGGGGC...GGCCTGGTACAGCCTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGACCAGCCTGGGA...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV3-33*05</td>\n",
       "      <td>IGHD3/OR15-3a*01</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCGAGAGACAAAAATTTGGGACTGGCCGGGAACTTCTTTGACT...</td>\n",
       "      <td>CARDKNLGLAGNFFDYW</td>\n",
       "      <td>...</td>\n",
       "      <td>304</td>\n",
       "      <td>313</td>\n",
       "      <td>326</td>\n",
       "      <td>367</td>\n",
       "      <td>35:T&gt;A,72:G&gt;T,92:G&gt;C,98:G&gt;A,132:G&gt;A,151:A&gt;T,15...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.046322</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGTCCAGCCTGGGA...</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGTCCAGCCTG...</td>\n",
       "      <td>CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGACCAGCCTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>IGHV3-74*02</td>\n",
       "      <td>IGHD1-26*01</td>\n",
       "      <td>IGHJ4*01</td>\n",
       "      <td>TGTGCAAGACAAGTGGGGGGCAATATCGACCACCTTTCGAAATACT...</td>\n",
       "      <td>CARQVGGNIDHLSKYYW</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>301</td>\n",
       "      <td>329</td>\n",
       "      <td>367</td>\n",
       "      <td>89:G&gt;C,93:C&gt;T,97:T&gt;C,119:C&gt;T,138:G&gt;A,147:A&gt;C,1...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...</td>\n",
       "      <td>GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence_id                                           sequence productive  \\\n",
       "0            0  CAGGTGCAGCTGCGGGAGTCGGGCCCAGGGCTGGTGAAGCCTTTGG...          T   \n",
       "1            1  CAGGTCACTTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...          T   \n",
       "2            2  GAGGTGCAGCTCCCGGAGTCTGGGGGCGGCCTGGTACAGCCTGGGG...          T   \n",
       "3            3  CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGACCAGCCTGGGA...          T   \n",
       "4            4  GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...          T   \n",
       "\n",
       "  stop_codon vj_in_frame       v_call            d_call    j_call  \\\n",
       "0          T           F  IGHV4-61*08        IGHD3-3*02  IGHJ4*01   \n",
       "1          T           F  IGHV2-70*19        IGHD2-8*02  IGHJ4*01   \n",
       "2          T           F  IGHV3-23*03       IGHD5-24*01  IGHJ4*01   \n",
       "3          T           F  IGHV3-33*05  IGHD3/OR15-3a*01  IGHJ4*01   \n",
       "4          T           F  IGHV3-74*02       IGHD1-26*01  IGHJ4*01   \n",
       "\n",
       "                                            junction            junction_aa  \\\n",
       "0  TGCGCGAGGCCGCCAGGTGTATCAGCATTTAGGAGGACACCCGCTT...  CARPPGVSAFRRTPAWDFDPW   \n",
       "1  TGTGCACGGGGGCATGTCCACGATAGGGTCTTTCCGAGAGTTGACT...      CARGHVHDRVFPRVDFW   \n",
       "2                  TGTGCGAGAGACGGAAAAAAGAGACCCGACTGG            CARDGKKRPDW   \n",
       "3  TGTGCGAGAGACAAAAATTTGGGACTGGCCGGGAACTTCTTTGACT...      CARDKNLGLAGNFFDYW   \n",
       "4  TGTGCAAGACAAGTGGGGGGCAATATCGACCACCTTTCGAAATACT...      CARQVGGNIDHLSKYYW   \n",
       "\n",
       "   ...  d_sequence_start  d_sequence_end j_sequence_start  j_sequence_end  \\\n",
       "0  ...               307             318              338             382   \n",
       "1  ...               310             313              329             370   \n",
       "2  ...               305             309              314             349   \n",
       "3  ...               304             313              326             367   \n",
       "4  ...               298             301              329             367   \n",
       "\n",
       "                                          shm_events  shm_count  shm_freq  \\\n",
       "0  14:A>G,30:A>G,44:C>T,68:C>A,84:C>T,89:G>C,90:C...         28  0.073298   \n",
       "1  9:C>T,72:C>T,82:T>C,85:C>A,88:A>T,100:A>G,105:...         20  0.054054   \n",
       "2  12:G>C,13:T>C,14:T>C,27:A>C,31:T>C,71:C>T,90:C...         34  0.097421   \n",
       "3  35:T>A,72:G>T,92:G>C,98:G>A,132:G>A,151:A>T,15...         17  0.046322   \n",
       "4  89:G>C,93:C>T,97:T>C,119:C>T,138:G>A,147:A>C,1...         14  0.038147   \n",
       "\n",
       "                                  unmutated_sequence  \\\n",
       "0  CAGGTGCAGCTGCAGGAGTCGGGCCCAGGACTGGTGAAGCCTTCGG...   \n",
       "1  CAGGTCACCTTGAGGGAGTCTGGTCCTGCGCTGGTGAAACCCACAC...   \n",
       "2  GAGGTGCAGCTGTTGGAGTCTGGGGGAGGCTTGGTACAGCCTGGGG...   \n",
       "3  CAGGTGCAGCTGGTGGAGTCTGGGGGAGGCGTGGTCCAGCCTGGGA...   \n",
       "4  GAGGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTTCAGCCTGGGG...   \n",
       "\n",
       "                           gapped_unmutated_sequence  \\\n",
       "0  CAGGTGCAGCTGCAGGAGTCGGGCCCA...GGACTGGTGAAGCCTT...   \n",
       "1  CAGGTCACCTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...   \n",
       "2  GAGGTGCAGCTGTTGGAGTCTGGGGGA...GGCTTGGTACAGCCTG...   \n",
       "3  CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGTCCAGCCTG...   \n",
       "4  GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...   \n",
       "\n",
       "                             gapped_mutated_sequence  \n",
       "0  CAGGTGCAGCTGCGGGAGTCGGGCCCA...GGGCTGGTGAAGCCTT...  \n",
       "1  CAGGTCACTTTGAGGGAGTCTGGTCCT...GCGCTGGTGAAACCCA...  \n",
       "2  GAGGTGCAGCTCCCGGAGTCTGGGGGC...GGCCTGGTACAGCCTG...  \n",
       "3  CAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCGTGGACCAGCCTG...  \n",
       "4  GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTAGTTCAGCCTG...  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./data/airrship_shm_seed42_10_000.tsv\"\n",
    "df_data = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ae4531-c3f0-403f-b8b1-5472bd2f3c33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "v_sequence_start    True\n",
       "v_sequence_end      True\n",
       "d_sequence_start    True\n",
       "d_sequence_end      True\n",
       "j_sequence_start    True\n",
       "j_sequence_end      True\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_names = [f\"{seg}_sequence_{pos}\" for seg in \"vdj\" for pos in (\"start\", \"end\")]\n",
    "sequences = df_data[\"sequence\"]\n",
    "positions = df_data[pos_names]\n",
    "\n",
    "display(sequences.notna().all())\n",
    "display(positions.notna().all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54249d2f-1e4b-478a-98f4-25fbd944aabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'CAGGTGCAGCTGCGGGAGTCGGGCCCAGGGCTGGTGAAGCCTTTGGAGACCCTGTCCCTCACCTGCAATGTCTCTGGTGGCTCTGTCACTAGTGGTGGTTACTACTGGAGTTGGGTCCGGCTGACCCCAGGGAAGGGACTGGACTGGATTGGTTTTCTTTATTACAGTGGGAGTACCAATTACAACCCCTCCCTCGAGACTCGAGTCACCATATCAGTAGACACGGCCAAGAACCAGTTCTCTCTGAAGGTGAGCTCTGTGACCGCTGCGGACACGGCCGTGTATTACTGCGCGAGGCCGCCAGGTGTATCAGCATTTAGGAGGACACCCGCTTGGGACTTTGACCCCTGGGGCCATGGAACCCTGGTCACCGTCTCCTCAG'>,\n",
       " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  1, 296, 307, 318, 338, 382])>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_seq = Dataset.from_tensor_slices(sequences.to_numpy())\n",
    "ds_pos = Dataset.from_tensor_slices(positions.to_numpy())\n",
    "ds_all = Dataset.zip((ds_seq, ds_pos))\n",
    "\n",
    "display(ds_all.take(1).get_single_element())\n",
    "display(ds_all.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26440fe-c4fc-4d68-8bf2-49be52fba8e7",
   "metadata": {},
   "source": [
    "### Functions for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2078e6a0-d0ae-4f65-869f-3fe525925a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dna_onehot_tensor(seq):\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "        initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant([\"A\", \"C\", \"G\", \"T\"], dtype=tf.string),\n",
    "            values=tf.constant([0, 1, 2, 3]),\n",
    "        ),\n",
    "        default_value=tf.constant(-1)\n",
    "    )\n",
    "    chars = tf.strings.bytes_split(seq)\n",
    "    ind = table.lookup(chars)\n",
    "    encoded = tf.one_hot(ind, depth=4)\n",
    "    return encoded\n",
    "\n",
    "# test dna_onehot_tensor\n",
    "# test_oh = ds_all.take(6).map(\n",
    "#     lambda x, y: (dna_onehot_tensor(x), y)\n",
    "# )\n",
    "# for padded_batch in test_oh.padded_batch(3):\n",
    "#     display(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3ed857-9c79-49b2-8dc7-460ead6006b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kmer_tensor(seq, k):\n",
    "    chars = tf.strings.bytes_split(seq)\n",
    "    kmers = tf.strings.ngrams(chars, k, separator=\"\")\n",
    "    sentence = tf.strings.reduce_join(kmers, separator=\" \")\n",
    "    return sentence\n",
    "\n",
    "# test get_kmer_tensor()\n",
    "# test_kmer = ds_all.take(3).map(\n",
    "#     lambda x, y: (get_kmer_tensor(x, 3), y)\n",
    "# )\n",
    "# display(test_kmer.batch(3).take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062118ea-6e54-49ef-b761-82791c8180b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kmer_vocab(k):\n",
    "    return [\"\".join(x) for x in product(\"ACGT\", repeat=k)]\n",
    "\n",
    "# test get_kmer_vocab()\n",
    "# vocab_kmer = get_kmer_vocab(3)\n",
    "# \" \".join(vocab_kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946328b-cc7e-4889-ab25-23f5eafb237a",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d574e8c0-5ceb-4fed-a2a2-356de9878bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 8000\n",
    "VALID_SIZE = 1000\n",
    "TEST_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a7c30d1-cb2d-41b8-bd71-559c87c59513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_all.take(TRAIN_SIZE)\n",
    "ds_not_train = ds_all.skip(TRAIN_SIZE)\n",
    "ds_valid = ds_not_train.take(VALID_SIZE)\n",
    "ds_test = ds_not_train.skip(VALID_SIZE)\n",
    "\n",
    "assert ds_train.cardinality().numpy() == TRAIN_SIZE\n",
    "assert ds_valid.cardinality().numpy() == VALID_SIZE\n",
    "assert ds_test.cardinality().numpy() == TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd8b5b8-9dda-4e59-b2c1-600cf868930c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_ds(ds, method, scale_factor, batch_size=None, shuffle_buffer=None, shuffle_seed=None):\n",
    "    assert method == \"onehot\" or method.endswith(\"mer\")\n",
    "    assert (shuffle_buffer is not None) ^ (shuffle_seed is None)\n",
    "    if method == \"onehot\":\n",
    "        ds = ds.map(lambda x, y: (dna_onehot_tensor(x), y / scale_factor))\n",
    "    else:\n",
    "        k = int(method[0])\n",
    "        ds = ds.map(lambda x, y: (get_kmer_tensor(x, k), y / scale_factor))\n",
    "    if not batch_size:\n",
    "        return ds\n",
    "    if shuffle_buffer:\n",
    "        ds = ds.shuffle(shuffle_buffer, seed=shuffle_seed, reshuffle_each_iteration=True)\n",
    "    batched = ds.padded_batch(batch_size) if method == \"onehot\" else ds.batch(batch_size)\n",
    "    return batched\n",
    "                    \n",
    "# # test encode_ds()\n",
    "# for batch_seq, batch_pos in transform_ds(ds_train.take(6), \"onehot\", 450, 3, 5, 1):\n",
    "#     display(batch_seq.shape)\n",
    "#     display(batch_pos.shape)   \n",
    "# display(batch_seq)\n",
    "# display(batch_pos)\n",
    "    \n",
    "# for batch_seq, batch_pos in transform_ds(ds_train.take(6), \"3mer\", 450, 3, 5, 1):\n",
    "#     display(batch_seq.shape)\n",
    "#     display(batch_pos.shape)\n",
    "# display(batch_seq)\n",
    "# display(batch_pos)\n",
    "\n",
    "# batched = transform_ds(ds_train, \"3mer\", 450, 32, 128, SEED)\n",
    "# display(batched.cardinality().numpy())\n",
    "# prefetched = batched.prefetch(8)\n",
    "# display(prefetched.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fec84b-7b68-4494-9f60-9cb9fb30eaa0",
   "metadata": {},
   "source": [
    "### Functions and classes for building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a3b0c7-ef2a-40e7-8839-3a64d2260d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRULNCell(keras.layers.GRUCell):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(units, **kwargs)\n",
    "        self.LN_x_to_gate = keras.layers.LayerNormalization(name=\"LN_x_to_gate\")\n",
    "        self.LN_h_to_gate = keras.layers.LayerNormalization(name=\"LN_h_to_gate\")\n",
    "        self.LN_x_to_recurrent = keras.layers.LayerNormalization(name=\"LN_x_to_recurrent\")\n",
    "        self.LN_h_to_recurrent = keras.layers.LayerNormalization(name=\"LN_h_to_recurrent\")\n",
    "        \n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_tm1 = states[0] if tf.nest.is_nested(states) else states  # previous memory\n",
    "\n",
    "        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "            h_tm1, training, count=3\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            if not self.reset_after:\n",
    "                input_bias, recurrent_bias = self.bias, None\n",
    "            else:\n",
    "                input_bias, recurrent_bias = tf.unstack(self.bias)\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0.0 < self.dropout < 1.0:\n",
    "                inputs_z = inputs * dp_mask[0]\n",
    "                inputs_r = inputs * dp_mask[1]\n",
    "                inputs_h = inputs * dp_mask[2]\n",
    "            else:\n",
    "                inputs_z = inputs\n",
    "                inputs_r = inputs\n",
    "                inputs_h = inputs\n",
    "\n",
    "            x_z = backend.dot(inputs_z, self.kernel[:, : self.units])\n",
    "            x_r = backend.dot(inputs_r, self.kernel[:, self.units : self.units * 2])\n",
    "            x_h = backend.dot(inputs_h, self.kernel[:, self.units * 2 :])\n",
    "            \n",
    "            # layer normalization: x to gate, x to recurrent\n",
    "            x_concat = backend.concatenate([x_z, x_r])\n",
    "            x_concat = self.LN_x_to_gate(x_concat)\n",
    "            x_z = x_concat[:, :self.units]\n",
    "            x_r = x_concat[:, self.units:]\n",
    "            x_h = self.LN_x_to_recurrent(x_h)\n",
    "\n",
    "            if self.use_bias:\n",
    "                x_z = backend.bias_add(x_z, input_bias[: self.units])\n",
    "                x_r = backend.bias_add(x_r, input_bias[self.units : self.units * 2])\n",
    "                x_h = backend.bias_add(x_h, input_bias[self.units * 2 :])\n",
    "\n",
    "            if 0.0 < self.recurrent_dropout < 1.0:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "\n",
    "            recurrent_z = backend.dot(\n",
    "                h_tm1_z, self.recurrent_kernel[:, : self.units]\n",
    "            )\n",
    "            recurrent_r = backend.dot(\n",
    "                h_tm1_r, self.recurrent_kernel[:, self.units : self.units * 2]\n",
    "            )\n",
    "            \n",
    "            # layer normalization: h to gate\n",
    "            recurrent_concat = backend.concatenate([recurrent_z, recurrent_r])\n",
    "            recurrent_concat = self.LN_h_to_gate(recurrent_concat)\n",
    "            recurrent_z = recurrent_concat[:, :self.units]\n",
    "            recurrent_r = recurrent_concat[:, self.units:]\n",
    "            \n",
    "            if self.reset_after and self.use_bias:\n",
    "                recurrent_z = backend.bias_add(\n",
    "                    recurrent_z, recurrent_bias[: self.units]\n",
    "                )\n",
    "                recurrent_r = backend.bias_add(\n",
    "                    recurrent_r, recurrent_bias[self.units : self.units * 2]\n",
    "                )\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            # reset gate applied after/before matrix multiplication\n",
    "            if self.reset_after:\n",
    "                recurrent_h = backend.dot(\n",
    "                    h_tm1_h, self.recurrent_kernel[:, self.units * 2 :]\n",
    "                )\n",
    "                # layer normalization: h to recurrent\n",
    "                recurrent_h = self.LN_h_to_recurrent(recurrent_h)\n",
    "                if self.use_bias:\n",
    "                    recurrent_h = backend.bias_add(\n",
    "                        recurrent_h, recurrent_bias[self.units * 2 :]\n",
    "                    )\n",
    "                recurrent_h = r * recurrent_h\n",
    "            else:\n",
    "                recurrent_h = backend.dot(\n",
    "                    r * h_tm1_h, self.recurrent_kernel[:, self.units * 2 :]\n",
    "                )\n",
    "                # layer normalization: h to recurrent\n",
    "                recurrent_h = self.LN_h_to_recurrent(recurrent_h)\n",
    "\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "        else:\n",
    "            if 0.0 < self.dropout < 1.0:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "\n",
    "            # inputs projected by all gate matrices at once\n",
    "            matrix_x = backend.dot(inputs, self.kernel)\n",
    "            \n",
    "            # layer normalization: x to gate, x to recurrent\n",
    "            x_concat = matrix_x[:, : self.units * 2]\n",
    "            x_h = matrix_x[:, self.units * 2: ]\n",
    "            x_concat = self.LN_x_to_gate(x_concat)\n",
    "            x_h = self.LN_x_to_recurrent(x_h)\n",
    "            matrix_x = backend.concatenate([x_concat, x_h])\n",
    "            \n",
    "            if self.use_bias:\n",
    "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
    "                matrix_x = backend.bias_add(matrix_x, input_bias)\n",
    "\n",
    "            x_z, x_r, x_h = tf.split(matrix_x, 3, axis=-1)\n",
    "\n",
    "            if self.reset_after:\n",
    "                # hidden state projected by all gate matrices at once\n",
    "                matrix_inner = backend.dot(h_tm1, self.recurrent_kernel)\n",
    "                \n",
    "                # layer normalization: h to gate, h to recurrent\n",
    "                recurrent_concat = matrix_inner[:, : self.units * 2]\n",
    "                recurrent_h = matrix_inner[:, self.units * 2: ]\n",
    "                recurrent_concat = self.LN_h_to_gate(recurrent_concat)\n",
    "                recurrent_h = self.LN_h_to_recurrent(recurrent_h)\n",
    "                matrix_inner = backend.concatenate([recurrent_concat, recurrent_h])\n",
    "                \n",
    "                if self.use_bias:\n",
    "                    matrix_inner = backend.bias_add(\n",
    "                        matrix_inner, recurrent_bias\n",
    "                    )\n",
    "            else:\n",
    "                # hidden state projected separately for update/reset and new\n",
    "                matrix_inner = backend.dot(\n",
    "                    h_tm1, self.recurrent_kernel[:, : 2 * self.units]\n",
    "                )\n",
    "                # layer normalization: h to gate\n",
    "                matrix_inner = self.LN_h_to_gate(matrix_inner)\n",
    "\n",
    "            recurrent_z, recurrent_r, recurrent_h = tf.split(\n",
    "                matrix_inner, [self.units, self.units, -1], axis=-1\n",
    "            )\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            if self.reset_after:\n",
    "                recurrent_h = r * recurrent_h\n",
    "            else:\n",
    "                recurrent_h = backend.dot(\n",
    "                    r * h_tm1, self.recurrent_kernel[:, 2 * self.units :]\n",
    "                )\n",
    "                # layer normalization: h to recurrent\n",
    "                recurrent_h = self.LN_h_to_recurrent(recurrent_h)\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "            \n",
    "        # previous and candidate state mixed by update gate\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        new_state = [h] if tf.nest.is_nested(states) else h\n",
    "        return h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d910fe37-2466-4a27-9330-d001711bd39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelWriteGrad(keras.Model):\n",
    "    \n",
    "    def set_model(\n",
    "        self, logdir, write_grad, \n",
    "        scalars=[], scalar_freq=1, histogram_freq=1, update_freq=\"batch\", batch_per_epoch=None, \n",
    "        inspect_adam=False, inspect_clip=False, replace_nan=None, replace_inf=None\n",
    "    ):\n",
    "        # Functions for compute scalars of gradients\n",
    "        self.scalar_funs = {\n",
    "            \"abs_max\": lambda x: tf.reduce_max(tf.abs(x)),\n",
    "            \"abs_min\": lambda x: tf.reduce_min(tf.abs(x)),\n",
    "            \"norm\": lambda x: tf.norm(x)\n",
    "        }\n",
    "        \n",
    "        # check arguments\n",
    "        assert set(scalars).issubset(set(self.scalar_funs.keys()))\n",
    "        assert update_freq in (\"batch\", \"epoch\")\n",
    "        assert not (update_freq == \"epoch\" and batch_per_epoch is None)\n",
    "        \n",
    "        # initialize\n",
    "        self.step = 0\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/gradients\") if write_grad else None\n",
    "        self.write_grad = write_grad\n",
    "        self.scalars = scalars\n",
    "        self.scalar_freq = scalar_freq if update_freq == \"batch\" else scalar_freq * batch_per_epoch\n",
    "        self.histogram_freq = histogram_freq if update_freq == \"batch\" else histogram_freq * batch_per_epoch\n",
    "        self.update_freq = update_freq\n",
    "        self.batch_per_epoch = batch_per_epoch\n",
    "        self.inspect_clip = inspect_clip\n",
    "        self.replace_nan = tf.constant(replace_nan, dtype=tf.float32) if replace_nan else False\n",
    "        self.replace_inf = tf.constant(replace_inf, dtype=tf.float32) if replace_inf else False\n",
    "        \n",
    "        opt_param = self.optimizer.get_config()\n",
    "        self.inspect_adam = (inspect_adam and opt_param[\"name\"] == \"Adam\")\n",
    "        if self.inspect_adam:\n",
    "            var_num = len(self.trainable_variables)\n",
    "            self.m = [0 for _ in range(var_num)]\n",
    "            self.v = [0 for _ in range(var_num)]\n",
    "            self.lr = opt_param[\"learning_rate\"]\n",
    "            self.beta_1 = opt_param[\"beta_1\"]\n",
    "            self.beta_2 = opt_param[\"beta_2\"]\n",
    "            self.epsilon = opt_param[\"epsilon\"]\n",
    "        if self.inspect_clip:\n",
    "            self.clipvalue = opt_param.get(\"clipvalue\")\n",
    "            self.clipnorm = opt_param.get(\"clipnorm\")\n",
    "            \n",
    "    def write_value(self, values, value_name, trainable_vars):\n",
    "        with self.writer.as_default():\n",
    "            for val, var in zip(values, trainable_vars):\n",
    "                if isinstance(val, tf.IndexedSlices):\n",
    "                    val = tf.convert_to_tensor(val)\n",
    "                var_name = var.name.replace(':', '_')\n",
    "                var_name = re.sub(r\"/gru_cell_\\d+\", \"\", var_name)\n",
    "                if self.histogram_freq != 0 and self.step % self.histogram_freq == 0:\n",
    "                    tf.summary.histogram(f\"{var_name}/{value_name}\", val, step=self.step)\n",
    "                if self.scalars and self.scalar_freq != 0 and self.step % self.scalar_freq == 0:\n",
    "                    for scalar in self.scalars:\n",
    "                        sum_name = f\"{var_name}/{value_name}/{scalar}\"\n",
    "                        sum_val = self.scalar_funs[scalar](val)\n",
    "                        tf.summary.scalar(sum_name, sum_val, step=self.step)\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def train_step(self, data):\n",
    "        \n",
    "        # Compute gradients\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Check and process nan and Inf\n",
    "        for i in range(len(gradients)):\n",
    "            grad = gradients[i]\n",
    "            var = trainable_vars[i]\n",
    "            var_name = var.name.replace(':', '_')\n",
    "            var_name = re.sub(r\"/gru_cell_\\d+\", \"\", var_name)\n",
    "            \n",
    "            nan_bool = tf.math.is_nan(grad)\n",
    "            if self.write_grad and self.inspect_clip:\n",
    "                show_nan = f\"Batch {self.step}: Nan in gradients of {var_name}\"\n",
    "                tf.cond(tf.reduce_any(nan_bool), lambda: tf.print(show_nan), tf.no_op)\n",
    "            if self.replace_nan:\n",
    "                if isinstance(grad, tf.IndexedSlices):\n",
    "                    grad = tf.convert_to_tensor(grad)\n",
    "                grad = tf.where(nan_bool, self.replace_nan, grad)\n",
    "                gradients[i] = grad\n",
    "            \n",
    "            inf_bool = tf.math.is_inf(grad)\n",
    "            if self.write_grad and self.inspect_clip:\n",
    "                show_inf = f\"Batch {self.step}: Inf in gradients of {var_name}\"\n",
    "                tf.cond(tf.reduce_any(inf_bool), lambda: tf.print(show_inf), tf.no_op)\n",
    "            if self.replace_inf:\n",
    "                if isinstance(grad, tf.IndexedSlices):\n",
    "                    grad = tf.convert_to_tensor(grad)\n",
    "                pos_bool = inf_bool & (grad > 0)\n",
    "                neg_bool = inf_bool & (grad < 0)\n",
    "                grad = tf.where(pos_bool, self.replace_inf, grad)\n",
    "                grad = tf.where(neg_bool, -1 * self.replace_inf, grad)\n",
    "                gradients[i] = grad\n",
    "                \n",
    "            if self.write_grad and self.inspect_clip and self.clipvalue:\n",
    "                show_clip = f\"Batch {self.step}: clip value for gradients of {var_name}\"\n",
    "                clip_bool = tf.reduce_any(tf.abs(grad) > self.clipvalue)\n",
    "                tf.cond(clip_bool, lambda: tf.print(show_clip), tf.no_op)\n",
    "            if self.write_grad and self.inspect_clip and self.clipnorm:\n",
    "                show_clip = f\"Batch {self.step}: clip norm for gradients of {var_name}\"\n",
    "                clip_bool = tf.reduce_any(tf.norm(grad) > self.clipnorm)\n",
    "                tf.cond(clip_bool, lambda: tf.print(show_clip), tf.no_op)\n",
    "        \n",
    "        # Compute adam values\n",
    "        if self.write_grad and self.inspect_adam:\n",
    "            t = self.step + 1\n",
    "            lr = self.lr * tf.sqrt(1 - tf.pow(self.beta_2, t)) / (1 - tf.pow(self.beta_1, t))\n",
    "            var_num = len(self.trainable_variables)\n",
    "            root_v = [0 for i in range(var_num)]\n",
    "            weight_updates = [0 for i in range(var_num)]\n",
    "            for i in range(var_num):\n",
    "                grad = gradients[i]\n",
    "                if isinstance(grad, tf.IndexedSlices):\n",
    "                    grad = tf.convert_to_tensor(grad)\n",
    "                self.m[i] = self.m[i] * self.beta_1 + (1 - self.beta_1) * grad\n",
    "                self.v[i] = self.v[i] * self.beta_2 + (1 - self.beta_2) * tf.square(grad)\n",
    "                root_v[i] = tf.sqrt(self.v[i])\n",
    "                weight_updates[i] = lr * self.m[i] / (root_v[i] + self.epsilon)\n",
    "        \n",
    "        # Record gradients\n",
    "        if self.write_grad:\n",
    "            self.write_value(gradients, \"grads\", trainable_vars)\n",
    "            if self.inspect_adam:\n",
    "                self.write_value(self.m, \"m\", trainable_vars)\n",
    "                self.write_value(root_v, \"root_v\", trainable_vars)\n",
    "                self.write_value(weight_updates, \"updates\", trainable_vars)\n",
    "                \n",
    "        # Update step, weights and metrics\n",
    "        self.step += 1\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72998de3-909a-4860-99c4-872693db0f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the encoders for kmer\n",
    "def create_kmer_encoder(k, embed_dim):\n",
    "    text_vec_layer = keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        split=\"whitespace\",\n",
    "        vocabulary=get_kmer_vocab(k),\n",
    "        name=\"text_vectorize\"\n",
    "    )\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        input_dim=text_vec_layer.vocabulary_size(), \n",
    "        output_dim=embed_dim, \n",
    "        mask_zero=True,\n",
    "        name=\"embedding\"\n",
    "    )\n",
    "\n",
    "    kmer_seq = keras.Input(shape=(1,), dtype=tf.string, name=f\"{k}mer_seq\")\n",
    "    tokens = text_vec_layer(kmer_seq)\n",
    "    embedding = embedding_layer(tokens)\n",
    "    kmer_encoder = keras.models.Model(kmer_seq, embedding, name=f\"{k}mer_encoder\")\n",
    "    return kmer_encoder\n",
    "    \n",
    "# test_encoder = create_kmer_encoder(5, 64)\n",
    "# test_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c9cc3d1-d5f2-4436-abb3-227db473f61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the complete models\n",
    "def create_model(\n",
    "    encode_name, *, \n",
    "    EMBED_DIM, GRU_NUM, GRU_UNIT, GRU_BIDIRECT, GRU_LN, \n",
    "    DENSE_NUM, DENSE_UNIT, LEAKY_ALPHA, DENSE_NORM, OUT_ACTI\n",
    "):\n",
    "    \n",
    "    # input and encoding part\n",
    "    if encode_name == \"onehot\":\n",
    "        model_input = keras.Input(shape=(None, 4), name=\"input\")\n",
    "        x = keras.layers.Masking(mask_value=[0.0, 0.0, 0.0, 0.0], name=\"mask\")(model_input)\n",
    "    else:\n",
    "        k = int(encode_name[0])\n",
    "        model_input = keras.Input(shape=(1, ), dtype=tf.string, name=\"input\")\n",
    "        kmer_encoder = create_kmer_encoder(k, EMBED_DIM)\n",
    "        x = kmer_encoder(model_input)\n",
    "    \n",
    "    # gru part\n",
    "    for i in range(GRU_NUM):\n",
    "        gru_unit = GRU_UNIT if isinstance(GRU_UNIT, int) else GRU_UNIT[i]\n",
    "        gru_bidirect = GRU_BIDIRECT if isinstance(GRU_BIDIRECT, bool) else GRU_BIDIRECT[i]\n",
    "        gru_ln = GRU_LN if isinstance(GRU_LN, bool) or isinstance(GRU_LN, str) else GRU_LN[i]\n",
    "        assert gru_ln in (\"inside\", \"outside\", False)\n",
    "        not_last_gru = (i+1 != GRU_NUM)\n",
    "        if gru_ln == \"inside\":\n",
    "            gru_ln_cell = GRULNCell(gru_unit)\n",
    "            gru_layer = keras.layers.RNN(gru_ln_cell, return_sequences=not_last_gru, name=f\"gru_ln_{i+1}\")\n",
    "        else:\n",
    "            gru_layer = keras.layers.GRU(gru_unit, return_sequences=not_last_gru, name=f\"gru_{i+1}\")\n",
    "        if gru_bidirect:\n",
    "            gru_layer = keras.layers.Bidirectional(gru_layer, name=f\"gru_{i+1}\")\n",
    "        x = gru_layer(x)\n",
    "        if gru_ln == \"outside\":\n",
    "            x = keras.layers.LayerNormalization(name=f\"gru_ln_{i+1}\")(x)\n",
    "    \n",
    "    # dense part\n",
    "    # dense_acti = lambda x: keras.activations.relu(x, alpha=LEAKY_ALPHA)\n",
    "    for i in range(DENSE_NUM):\n",
    "        dense_unit = DENSE_UNIT if isinstance(DENSE_UNIT, int) else DENSE_UNIT[i]\n",
    "        dense_norm = DENSE_NORM if isinstance(DENSE_NORM, bool) or isinstance(DENSE_NORM, str) else DENSE_NORM[i]\n",
    "        assert dense_norm in (\"BN\", \"LN\", False)\n",
    "        if dense_norm == \"BN\":\n",
    "            x = keras.layers.Dense(dense_unit, use_bias=False, activation=None, name=f\"dense_{i+1}\")(x)\n",
    "            x = keras.layers.BatchNormalization(name=f\"dense_bn_{i+1}\")(x)\n",
    "        elif dense_norm == \"LN\":\n",
    "            x = keras.layers.Dense(dense_unit, use_bias=True, activation=None, name=f\"dense_{i+1}\")(x)\n",
    "            x = keras.layers.LayerNormalization(name=f\"dense_ln_{i+1}\")(x)\n",
    "        else:\n",
    "            x = keras.layers.Dense(dense_unit, use_bias=True, activation=None, name=f\"dense_{i+1}\")(x)\n",
    "        x = keras.layers.LeakyReLU(alpha=LEAKY_ALPHA, name=f\"dense_leaky_relu_{i+1}\")(x)\n",
    "    \n",
    "    # build the model\n",
    "    model_output = keras.layers.Dense(6, activation=OUT_ACTI, name=\"output\")(x)\n",
    "    model = ModelWriteGrad(model_input, model_output, name=f\"{encode_name}_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e3078-0953-4538-8305-c928dea890fe",
   "metadata": {},
   "source": [
    "### Build model, compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4928c433-41a7-4e99-afae-bae89ca5c229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretty_json(hp):\n",
    "    json_hp = json.dumps(hp, indent=2)\n",
    "    return \"\".join(\"\\t\" + line for line in json_hp.splitlines(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee95f8c7-745b-4df9-9c0e-120df039012f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom metrics\n",
    "class ScaledRMSE(keras.metrics.RootMeanSquaredError):\n",
    "    def __init__(self, scale_factor, name=\"s_rmse\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.cast(y_pred, self._dtype) * self.scale_factor\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class ScaledMAE(keras.metrics.MeanAbsoluteError):\n",
    "    def __init__(self, scale_factor, name=\"s_mae\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.cast(y_pred, self._dtype) * self.scale_factor\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class PosAccuracy(keras.metrics.Accuracy):\n",
    "    def __init__(self, scale_factor, name=\"p_acc\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.math.rint(tf.cast(y_pred, self._dtype) * self.scale_factor)\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class AllAccuracy(keras.metrics.Accuracy):\n",
    "    def __init__(self, scale_factor, name=\"a_acc\", dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.math.rint(tf.cast(y_pred, self._dtype) * self.scale_factor)\n",
    "        all_correct_bool = tf.reduce_all(y_true == y_pred, axis=-1)\n",
    "        correct_mat = tf.where(all_correct_bool, 1, 0)\n",
    "        y_pred = tf.reshape(correct_mat, (-1, 1))\n",
    "        y_true = tf.ones(tf.shape(y_true)[0], 1)\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "        \n",
    "class ClassAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, scale_factor, name=\"c_acc\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.len_sum = self.add_weight(name=\"len_sum\", initializer=\"zeros\")\n",
    "        self.correct_sum = self.add_weight(name=\"correct_sum\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.rint(tf.cast(y_true, self._dtype) * self.scale_factor)\n",
    "        y_pred = tf.math.rint(tf.cast(y_pred, self._dtype) * self.scale_factor)\n",
    "        incorrect_count = tf.reduce_sum(tf.abs(y_true - y_pred))\n",
    "        batch_len_sum = tf.reduce_sum(tf.gather(y_true, indices=(5), axis=-1))\n",
    "        self.len_sum.assign_add(batch_len_sum)\n",
    "        self.correct_sum.assign_add(batch_len_sum - incorrect_count)\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_sum / self.len_sum\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.len_sum.assign(0.0)\n",
    "        self.correct_sum.assign(0.0)\n",
    "        \n",
    "class SegmentPosAccuracy(PosAccuracy):\n",
    "    def __init__(self, scale_factor, segment, name=\"p_acc\", dtype=None):\n",
    "        assert segment in (\"V\", \"D\", \"J\")\n",
    "        super().__init__(scale_factor, name=f\"{segment}_{name}\", dtype=dtype)\n",
    "        self.segment = segment\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        seg_idx_dict = {\"V\": (0, 1), \"D\": (2, 3), \"J\": (4, 5)}\n",
    "        seg_idx = seg_idx_dict[self.segment]\n",
    "        y_true_seg = tf.gather(y_true, indices=seg_idx, axis=-1)\n",
    "        y_pred_seg = tf.gather(y_pred, indices=seg_idx, axis=-1)\n",
    "        super().update_state(y_true_seg, y_pred_seg, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e402306-df34-409c-9701-92ce66299250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearWarmup(keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        after_warmup_lr_sched,\n",
    "        warmup_steps,\n",
    "        warmup_learning_rate,\n",
    "        after_warmup_offset=None,\n",
    "        name=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._name = name\n",
    "        self._after_warmup_lr_sched = after_warmup_lr_sched\n",
    "        if isinstance(after_warmup_offset, int) and after_warmup_offset <= warmup_steps:\n",
    "            self._after_warmup_offset = after_warmup_offset\n",
    "        elif after_warmup_offset is None:\n",
    "            self._after_warmup_offset = warmup_steps\n",
    "        else:\n",
    "            raise TypeError(\"after_warmup_offset must be int or None!\")\n",
    "        self._warmup_steps = warmup_steps\n",
    "        self._init_warmup_lr = warmup_learning_rate\n",
    "        if isinstance(after_warmup_lr_sched, keras.optimizers.schedules.LearningRateSchedule):\n",
    "            self._final_warmup_lr = after_warmup_lr_sched(self._warmup_steps - self._after_warmup_offset)\n",
    "        else:\n",
    "            self._final_warmup_lr = tf.cast(after_warmup_lr_sched, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "\n",
    "        global_step = tf.cast(step, dtype=tf.float32)\n",
    "\n",
    "        linear_warmup_lr = (\n",
    "            self._init_warmup_lr + global_step / self._warmup_steps *\n",
    "            (self._final_warmup_lr - self._init_warmup_lr)\n",
    "        )\n",
    "\n",
    "        if isinstance(self._after_warmup_lr_sched, keras.optimizers.schedules.LearningRateSchedule):\n",
    "            after_warmup_lr = self._after_warmup_lr_sched(step - self._after_warmup_offset)\n",
    "        else:\n",
    "            after_warmup_lr = tf.cast(self._after_warmup_lr_sched, dtype=tf.float32)\n",
    "\n",
    "        lr = tf.where(global_step < self._warmup_steps, linear_warmup_lr, after_warmup_lr)\n",
    "        return lr\n",
    "\n",
    "    def get_config(self):\n",
    "        if isinstance(self._after_warmup_lr_sched, keras.optimizers.schedules.LearningRateSchedule):\n",
    "            config = {\n",
    "                \"after_warmup_lr_sched\": self._after_warmup_lr_sched.get_config()\n",
    "            }  \n",
    "        else:\n",
    "            config = {\"after_warmup_lr_sched\": self._after_warmup_lr_sched}\n",
    "\n",
    "        config.update({\n",
    "            \"after_warmup_offset\": self._after_warmup_offset,\n",
    "            \"warmup_steps\": self._warmup_steps,\n",
    "            \"warmup_learning_rate\": self._init_warmup_lr,\n",
    "            \"name\": self._name\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "# test_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "#     1e-2, decay_steps=1000 - 100, alpha=1e-6, name=\"cos_decay\"\n",
    "# )\n",
    "# test_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "#     1e-2,\n",
    "#     batch_per_epoch,\n",
    "#     t_mul=1.0,\n",
    "#     m_mul=0.95,\n",
    "#     alpha=1e-6,\n",
    "#     name=\"cos_decay_restart\"\n",
    "# )\n",
    "# test_schedule = LinearWarmup(test_schedule, after_warmup_offset=20, warmup_steps=100, warmup_learning_rate=1e-6)\n",
    "# plt.plot(test_schedule(tf.range(1000)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3cb6c61-2437-4912-a79c-9af101ae6b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A callback class that stops the training when there is a sudden increase in one of the metrics \n",
    "class SpikeStopping(keras.callbacks.Callback):\n",
    "    def __init__(self, monitor, fold_allow=2, min_allow=5, skip_first=10):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.fold_allow = fold_allow\n",
    "        self.min_allow = min_allow\n",
    "        self.skip_first = skip_first\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if epoch == 0:\n",
    "            self.last = current\n",
    "        if epoch >= self.skip_first and current > self.last * fold_allow + min_allow:\n",
    "            print(f\"Stop training at epoch {epoch}\")\n",
    "            self.model.stop_training = True\n",
    "        self.last = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e1ef42a-d2dc-4dc5-bc0c-b007888fa230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A callback class that records the training information\n",
    "class WriteInfo(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir, info):\n",
    "        super().__init__()\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/info\")\n",
    "        self.info = info\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        model_info = {\n",
    "            \"MODEL_NAME\": self.model.name,\n",
    "            \"MODEL_PARAM_NUM\": self.model.count_params(),\n",
    "        }\n",
    "        self.info[\"MODEL_INFO\"] = model_info\n",
    "        info_json = pretty_json(self.info)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.text(\"training_setting\", info_json, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dd17fa8-8840-4ef4-b322-78f3eb287ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A callback class that records the training time\n",
    "class WriteTrainingTime(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir, print_time=True):\n",
    "        super().__init__()\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/info\")\n",
    "        self.print_time = print_time\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start = datetime.now()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        duration = datetime.now() - self.start + datetime.min\n",
    "        time_str = duration.strftime(\"%H:%M:%S.%f\")\n",
    "        if self.print_time:\n",
    "            print(f\"Time usage: {time_str}\")\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.text(\"training_time\", time_str, step=self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9db5b2a-69cc-4b16-92cb-9bce5cffab7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WriteLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, logdir):\n",
    "        super().__init__()\n",
    "        self.writer = tf.summary.create_file_writer(logdir + \"/lr\")\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = keras.backend.eval(self.model.optimizer.learning_rate)\n",
    "        step = keras.backend.eval(self.model.optimizer.iterations)\n",
    "        if isinstance(lr, keras.optimizers.schedules.LearningRateSchedule):\n",
    "            val = lr(step)\n",
    "        else:\n",
    "            val = lr\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(\"learning_rate\", val, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1dceabc-8fef-46e7-ab40-62e3fdb9d780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRU_UNIT</th>\n",
       "      <th>GRU_NUM</th>\n",
       "      <th>GRU_BIDIRECT</th>\n",
       "      <th>DENSE_UNIT</th>\n",
       "      <th>DENSE_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GRU_UNIT GRU_NUM GRU_BIDIRECT DENSE_UNIT DENSE_NUM\n",
       "1     1024       1        False        128         4\n",
       "2      512       1         True        128         4\n",
       "3      256       6        False        256         1\n",
       "4      128       6         True        256         1\n",
       "5      256       3         True        256         4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "    \"GRU_UNIT\": [1024, 512, 256, 128, 256],\n",
    "    \"GRU_NUM\": [1, 1, 6, 6, 3], \n",
    "    \"GRU_BIDIRECT\": [False, True, False, True, True], \n",
    "    \"DENSE_UNIT\": [128, 128, 256, 256, 256], \n",
    "    \"DENSE_NUM\": [4, 4, 1, 1, 4]\n",
    "}\n",
    "df_settings = pd.DataFrame(settings, index=range(1, 6), dtype=\"object\")\n",
    "df_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cab5e88d-9965-44ab-bf7f-720c062ced81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"5mer_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " 5mer_encoder (Functional)   (None, None, 64)          65664     \n",
      "                                                                 \n",
      " gru_1 (Bidirectional)       (None, None, 512)         494592    \n",
      "                                                                 \n",
      " gru_2 (Bidirectional)       (None, None, 512)         1182720   \n",
      "                                                                 \n",
      " gru_3 (Bidirectional)       (None, 512)               1182720   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_leaky_relu_1 (LeakyRe  (None, 256)              0         \n",
      " LU)                                                             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_leaky_relu_2 (LeakyRe  (None, 256)              0         \n",
      " LU)                                                             \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_leaky_relu_3 (LeakyRe  (None, 256)              0         \n",
      " LU)                                                             \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_leaky_relu_4 (LeakyRe  (None, 256)              0         \n",
      " LU)                                                             \n",
      "                                                                 \n",
      " output (Dense)              (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,255,942\n",
      "Trainable params: 3,255,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SETTING = 5\n",
    "MODEL_STRUCT = {\n",
    "    \"EMBED_DIM\": 64,\n",
    "    \"GRU_UNIT\": df_settings.loc[SETTING, \"GRU_UNIT\"],\n",
    "    \"GRU_NUM\": df_settings.loc[SETTING, \"GRU_NUM\"],\n",
    "    \"GRU_BIDIRECT\": df_settings.loc[SETTING, \"GRU_BIDIRECT\"],\n",
    "    \"GRU_LN\": False,\n",
    "    \"DENSE_UNIT\": df_settings.loc[SETTING, \"DENSE_UNIT\"],\n",
    "    \"DENSE_NUM\": df_settings.loc[SETTING, \"DENSE_NUM\"],\n",
    "    \"DENSE_NORM\": False,\n",
    "    \"LEAKY_ALPHA\": 0.1,\n",
    "    \"OUT_ACTI\": \"linear\"\n",
    "}\n",
    "show_model = create_model(\"5mer\", **MODEL_STRUCT)\n",
    "show_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a079a205-efeb-4425-8a95-0f52e367ceab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training setting\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 32\n",
    "SCALE_FACTOR = 450\n",
    "SHUFFLE_BUFFER = 128\n",
    "PREFETCH_BUFFER = 8\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "REPLACE_NAN = 1e-5\n",
    "REPLACE_INF = 1e-5\n",
    "EARLY_STOP = {\"monitor\": \"loss\", \"min_delta\": 0, \"patience\": 20}\n",
    "# EARLY_STOP = False\n",
    "STOP_NAN = True\n",
    "WRITE_GRAD = False\n",
    "\n",
    "train_info = {\n",
    "    \"SEED\": SEED,\n",
    "    \"DATA\": {\n",
    "        \"DATA_PATH\": DATA_PATH,\n",
    "        \"TRAIN_SIZE\": TRAIN_SIZE,\n",
    "        \"VALID_SIZE\": VALID_SIZE,\n",
    "        \"TEST_SIZE\": TEST_SIZE,\n",
    "        \"SCALE_FACTOR\": SCALE_FACTOR,\n",
    "        \"SHUFFLE_BUFFER\": SHUFFLE_BUFFER,\n",
    "        \"PREFETCH_BUFFER\": PREFETCH_BUFFER\n",
    "    },\n",
    "    \"MODEL_STRUCT\": MODEL_STRUCT,\n",
    "    \"TRAIN_SET\": {\n",
    "        \"EPOCH\": EPOCH,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LOSS\": LOSS.get_config(),\n",
    "        \"REPLACE_NAN\": REPLACE_NAN,\n",
    "        \"REPLACE_INF\": REPLACE_INF,\n",
    "        \"EARLY_STOP\": EARLY_STOP,\n",
    "        \"STOP_NAN\": STOP_NAN,\n",
    "        \"WRITE_GRAD\": WRITE_GRAD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4d22464-76ef-4f08-8343-cb27fe28bfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear_warmup'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_SCHEDULE = LinearWarmup(\n",
    "    LR_SCHEDULE, \n",
    "    warmup_steps=batch_per_epoch * WARMUP_EPOCH, \n",
    "    warmup_learning_rate=1e-6,\n",
    "    name=\"linear_warmup\"\n",
    ")\n",
    "LR_SCHEDULE.get_config()[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "099d1a77-d5c4-4319-909a-640ec0fc1ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5mer'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_1/kernel:0' shape=(256, 256) dtype=float32, numpy=\n",
       "array([[ 0.09940689, -0.04302669,  0.06853006, ...,  0.04871876,\n",
       "        -0.01533163, -0.07063694],\n",
       "       [ 0.08925147,  0.00025051, -0.08533851, ..., -0.09536003,\n",
       "         0.04990416,  0.02982887],\n",
       "       [ 0.04703335,  0.09673565,  0.02113684, ..., -0.09359347,\n",
       "         0.10377292, -0.01580728],\n",
       "       ...,\n",
       "       [-0.05997573,  0.05840721,  0.0018357 , ..., -0.09509703,\n",
       "        -0.10667745,  0.09354789],\n",
       "       [-0.00074476, -0.09086062, -0.05029048, ..., -0.03969767,\n",
       "        -0.07108419,  0.05995592],\n",
       "       [-0.02960476, -0.01729961, -0.0616333 , ...,  0.06001755,\n",
       "         0.03784332, -0.10430547]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5mer_model training!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 109\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mset_model(\n\u001b[1;32m    104\u001b[0m     logdir, write_grad\u001b[38;5;241m=\u001b[39mWRITE_GRAD,\n\u001b[1;32m    105\u001b[0m     scalars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m], scalar_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m    106\u001b[0m     update_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_per_epoch\u001b[38;5;241m=\u001b[39mbatch_per_epoch,\n\u001b[1;32m    107\u001b[0m     inspect_adam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inspect_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, replace_nan\u001b[38;5;241m=\u001b[39mREPLACE_NAN, replace_inf\u001b[38;5;241m=\u001b[39mREPLACE_INF\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m all_history[name] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_train_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_valid_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# model.save(logdir + \"/model\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "timestamp = datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "MAIN_LOGDIR = f\"./logs/lr_schedule/chk_repro\"\n",
    "\n",
    "# encode_names = (\"3mer\", \"4mer\", \"5mer\", )\n",
    "encode_names = (\"5mer\", )\n",
    "metric_list = [\n",
    "    ScaledRMSE(SCALE_FACTOR),\n",
    "    ScaledMAE(SCALE_FACTOR),\n",
    "    PosAccuracy(SCALE_FACTOR),\n",
    "    AllAccuracy(SCALE_FACTOR),\n",
    "    ClassAccuracy(SCALE_FACTOR),\n",
    "    SegmentPosAccuracy(SCALE_FACTOR, segment=\"V\"),\n",
    "    SegmentPosAccuracy(SCALE_FACTOR, segment=\"D\"),\n",
    "    SegmentPosAccuracy(SCALE_FACTOR, segment=\"J\")\n",
    "]\n",
    "\n",
    "all_history = {}\n",
    "for name in (\"onehot\", \"2mer\", \"3mer\", \"4mer\", \"5mer\"):\n",
    "    \n",
    "    # build model and compile\n",
    "    model = create_model(name, **MODEL_STRUCT)  # must build model first to ensure the initialization is the same\n",
    "    if name not in encode_names:\n",
    "        continue\n",
    "    # display(name)\n",
    "    # display(model.get_layer(\"dense_1\").weights[0])\n",
    "\n",
    "    # prepare dataset\n",
    "    ds_train_input = transform_ds(ds_train, name, SCALE_FACTOR, BATCH_SIZE, SHUFFLE_BUFFER, SEED)\n",
    "    ds_valid_input = transform_ds(ds_valid, name, SCALE_FACTOR, BATCH_SIZE)\n",
    "    ds_train_input = ds_train_input.prefetch(PREFETCH_BUFFER)\n",
    "    ds_valid_input = ds_valid_input.prefetch(PREFETCH_BUFFER)\n",
    "    batch_per_epoch = int(ds_train_input.cardinality().numpy())\n",
    "    \n",
    "    # compile\n",
    "    WARMUP_EPOCH = 3\n",
    "    DECAY_EPOCH = 250\n",
    "    LR_SCHEDULE = 1e-3\n",
    "    # LR_SCHEDULE = keras.optimizers.schedules.CosineDecay(\n",
    "    #     1e-3, decay_steps=batch_per_epoch * DECAY_EPOCH, alpha=0.3, name=\"cos_decay\"\n",
    "    # )\n",
    "\n",
    "#     LR_SCHEDULE = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "#         1e-3,\n",
    "#         batch_per_epoch,\n",
    "#         t_mul=2.0,\n",
    "#         m_mul=0.95,\n",
    "#         alpha=1e-3,\n",
    "#         name=\"cos_decay_restart\"\n",
    "#     )\n",
    "\n",
    "#     LR_SCHEDULE = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         1e-3, \n",
    "#         decay_steps=batch_per_epoch * (DECAY_EPOCH // int(log(0.3, 0.95))), \n",
    "#         decay_rate=0.95, \n",
    "#         staircase=True, \n",
    "#         name=\"exp_decay\"\n",
    "#     )\n",
    "    \n",
    "    LR_SCHEDULE = LinearWarmup(\n",
    "        LR_SCHEDULE, \n",
    "        warmup_steps=batch_per_epoch * WARMUP_EPOCH, \n",
    "        warmup_learning_rate=1e-6,\n",
    "        name=\"linear_warmup\"\n",
    "    )\n",
    "    OPTIMIZER = keras.optimizers.Adam(\n",
    "        learning_rate=LR_SCHEDULE,\n",
    "        clipnorm=1e-3\n",
    "    )\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=metric_list, run_eagerly=WRITE_GRAD)\n",
    "    if isinstance(LR_SCHEDULE, float):\n",
    "        opt_info = {\"OPTIMIZER\": OPTIMIZER.get_config()}\n",
    "    else:\n",
    "        opt_info = {\"WARMUP_EPOCH\": WARMUP_EPOCH, \"DECAY_EPOCH\": DECAY_EPOCH, \"OPTIMIZER\": OPTIMIZER.get_config()}\n",
    "    train_info[\"TRAIN_SET\"].update(opt_info)\n",
    "    \n",
    "    # setting callbacks\n",
    "    logdir = MAIN_LOGDIR + f\"/{model.name}\"\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "        logdir, \n",
    "        histogram_freq=max(1, EPOCH // 20),\n",
    "        write_graph=False\n",
    "    )\n",
    "    # spike_stop_cb = SpikeStopping(\"s_rmse\", fold_allow=1.5, min_allow=3, skip_first=10)\n",
    "    write_info_cb = WriteInfo(logdir, train_info)\n",
    "    write_time_cb = WriteTrainingTime(logdir)\n",
    "    write_lr_cb = WriteLearningRate(logdir)\n",
    "    cb_list = [\n",
    "        tensorboard_cb,\n",
    "        # spike_stop_cb,\n",
    "        write_info_cb,\n",
    "        write_time_cb,\n",
    "        write_lr_cb\n",
    "    ]\n",
    "    if STOP_NAN:\n",
    "        stop_nan_cb = keras.callbacks.TerminateOnNaN()\n",
    "        cb_list.append(stop_nan_cb)\n",
    "    if EARLY_STOP:\n",
    "        early_stop_cb = keras.callbacks.EarlyStopping(**EARLY_STOP)\n",
    "        cb_list.append(early_stop_cb)\n",
    "    \n",
    "    # train\n",
    "    print(f\"{model.name} training!\")\n",
    "    model.set_model(\n",
    "        logdir, write_grad=WRITE_GRAD,\n",
    "        scalars=[\"norm\"], scalar_freq=0, histogram_freq=0, \n",
    "        update_freq=\"epoch\", batch_per_epoch=batch_per_epoch,\n",
    "        inspect_adam=False, inspect_clip=False, replace_nan=REPLACE_NAN, replace_inf=REPLACE_INF\n",
    "    )\n",
    "    all_history[name] = model.fit(\n",
    "        ds_train_input,\n",
    "        epochs=EPOCH, \n",
    "        validation_data=ds_valid_input,\n",
    "        callbacks=cb_list,\n",
    "        verbose=0\n",
    "    )\n",
    "    # model.save(logdir + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff764c-8354-4faa-a41d-3a7ba900a0c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bbbb1-c29e-44f0-b0b9-d712a20168ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance = pd.DataFrame()\n",
    "for i, name in enumerate(encode_names):\n",
    "    df = pd.DataFrame(all_history[name].history)\n",
    "    best_loss_idx = df[\"loss\"].argmin()\n",
    "    best_val_loss_idx = df[\"val_loss\"].argmin()\n",
    "    valid_metrics_idx = df.columns.str.startswith(\"val\")\n",
    "    train_metrics = df.columns[~valid_metrics_idx]\n",
    "    valid_metrics = df.columns[valid_metrics_idx]\n",
    "    performance.loc[i, \"name\"] = name\n",
    "    performance.loc[i, \"number_of_parameters\"] = all_history[name].model.count_params()\n",
    "    performance.loc[i, \"best_loss_epoch\"] = df.index[best_loss_idx]\n",
    "    performance.loc[i, train_metrics] = df.loc[best_loss_idx, train_metrics]\n",
    "    performance.loc[i, \"best_val_loss_epoch\"] = df.index[best_val_loss_idx]\n",
    "    performance.loc[i, valid_metrics] = df.loc[best_loss_idx, valid_metrics]\n",
    "\n",
    "performance = pd.DataFrame(performance)\n",
    "performance.to_csv(MAIN_LOGDIR + \"/performance.csv\", index=False)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38eb29-f248-48d7-9a97-278bff60792b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name in encode_names:\n",
    "#     print(name)\n",
    "#     model = all_history[name].model\n",
    "#     ds_test_input = transform_ds(ds_test, name, SCALE_FACTOR, BATCH_SIZE)\n",
    "#     model.evaluate(ds_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5d8fc-7069-49d8-8dbc-ad5b0862ad57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DATA_PATH.endswith(\"10_000_processed.tsv\"):\n",
    "#     print(\"Train on random ends, test on not random ends\")\n",
    "#     df_other = pd.read_csv(\"./data/airrship_shm_seed42_10_000.tsv\", sep=\"\\t\")\n",
    "# else:\n",
    "#     print(\"Train on not random ends, test on random ends\")\n",
    "#     df_other = pd.read_csv(\"./data/airrship_shm_seed42_10_000_processed.tsv\", sep=\"\\t\")\n",
    "    \n",
    "# df_other = df_other.iloc[-1000:]\n",
    "# print(df_other.shape)\n",
    "\n",
    "# pos_names = [f\"{seg}_sequence_{pos}\" for seg in \"vdj\" for pos in (\"start\", \"end\")]\n",
    "# seq_other = df_other[\"sequence\"]\n",
    "# pos_other = df_other[pos_names]\n",
    "\n",
    "# ds_seq_other = Dataset.from_tensor_slices(seq_other.to_numpy())\n",
    "# ds_pos_other = Dataset.from_tensor_slices(pos_other.to_numpy())\n",
    "# ds_all_other = Dataset.zip((ds_seq_other, ds_pos_other))\n",
    "\n",
    "# for name in encode_names:\n",
    "#     print(name)\n",
    "#     model = all_history[name].model\n",
    "#     ds_other = transform_ds(ds_all_other, name, SCALE_FACTOR, BATCH_SIZE)\n",
    "#     model.evaluate(ds_other)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf210",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
